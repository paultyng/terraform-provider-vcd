package vcd

import (
	"bytes"
	"context"
	_ "embed"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"github.com/hashicorp/go-cty/cty"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"github.com/vmware/go-vcloud-director/v2/govcd"
	"github.com/vmware/go-vcloud-director/v2/types/v56"
	"github.com/vmware/go-vcloud-director/v2/util"
	"gopkg.in/yaml.v2"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"text/template"
	"time"
)

// supportedCseVersions is a map that contains only the supported CSE versions as keys,
// and its corresponding components versions as a slice of strings. The first string is the VCDKEConfig RDE Type version,
// then the CAPVCD RDE Type version and finally the CAPVCD Behavior version.
var supportedCseVersions = map[string][]string{
	"4.2": {
		"1.1.0", // VCDKEConfig RDE Type version
		"1.2.0", // CAPVCD RDE Type version
		"1.0.0", // CAPVCD Behavior version
	},
}

func resourceVcdCseKubernetesCluster() *schema.Resource {
	return &schema.Resource{
		CreateContext: resourceVcdCseKubernetesClusterCreate,
		ReadContext:   resourceVcdCseKubernetesRead,
		UpdateContext: resourceVcdCseKubernetesUpdate,
		DeleteContext: resourceVcdCseKubernetesDelete,
		Schema: map[string]*schema.Schema{
			"cse_version": {
				Type:         schema.TypeString,
				Required:     true,
				ForceNew:     true,
				ValidateFunc: validation.StringInSlice(getKeys(supportedCseVersions), false),
				Description:  "The CSE version to use",
			},
			"runtime": {
				Type:         schema.TypeString,
				Optional:     true,
				Default:      "tkg",
				ForceNew:     true,
				ValidateFunc: validation.StringInSlice([]string{"tkg"}, false), // May add others in future releases of CSE
				Description:  "The Kubernetes runtime for the cluster. Only 'tkg' (Tanzu Kubernetes Grid) is supported",
			},
			"name": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: "The name of the Kubernetes cluster",
				ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
					"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
			},
			"ova_id": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: "The ID of the vApp Template that corresponds to a Kubernetes template OVA",
			},
			"org": {
				Type:     schema.TypeString,
				Optional: true,
				ForceNew: true,
				Description: "The name of organization that will own this Kubernetes cluster, optional if defined at provider " +
					"level. Useful when connected as sysadmin working across different organizations",
			},
			"vdc_id": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: "The ID of the VDC that hosts the Kubernetes cluster",
			},
			"network_id": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: "The ID of the network that the Kubernetes cluster will use",
			},
			"owner": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: "The user that creates the cluster and owns the API token specified in 'api_token'. It must have the 'Kubernetes Cluster Author' role. If not specified, it assumes it's the user from the provider configuration",
			},
			"api_token_file": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: "A file generated by 'vcd_api_token' resource, that stores the API token used to create and manage the cluster, owned by the user specified in 'owner'. Be careful about this file, as it contains sensitive information",
			},
			"ssh_public_key": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: "The SSH public key used to login into the cluster nodes",
			},
			"control_plane": {
				Type:     schema.TypeList,
				MaxItems: 1,
				Required: true,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"machine_count": {
							Type:        schema.TypeInt,
							Optional:    true,
							Default:     3, // As suggested in UI
							Description: "The number of nodes that the control plane has. Must be an odd number and higher than 0",
							ValidateDiagFunc: func(v interface{}, path cty.Path) diag.Diagnostics {
								value, ok := v.(int)
								if !ok {
									return diag.Errorf("could not parse int value '%v' for control plane nodes", v)
								}
								if value < 1 || value%2 == 0 {
									return diag.Errorf("number of control plane nodes must be odd and higher than 0, but it was '%d'", value)
								}
								return nil
							},
						},
						"disk_size_gi": {
							Type:             schema.TypeInt,
							Optional:         true,
							Default:          20, // As suggested in UI
							ForceNew:         true,
							ValidateDiagFunc: minimumValue(20, "disk size in Gibibytes must be at least 20"),
							Description:      "Disk size, in Gibibytes, for the control plane nodes. Must be at least 20",
						},
						"sizing_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Sizing policy for the control plane nodes",
						},
						"placement_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Placement policy for the control plane nodes",
						},
						"storage_profile_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "Storage profile for the control plane nodes",
						},
						"ip": {
							Type:         schema.TypeString,
							Optional:     true,
							ForceNew:     true,
							Description:  "IP for the control plane",
							ValidateFunc: checkEmptyOrSingleIP(),
						},
					},
				},
			},
			"node_pool": {
				Type:     schema.TypeSet,
				Required: true,
				MinItems: 1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"name": {
							Type:        schema.TypeString,
							Required:    true,
							Description: "The name of this node pool",
							ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
								"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
						},
						"machine_count": {
							Type:             schema.TypeInt,
							Optional:         true,
							Default:          1, // As suggested in UI
							Description:      "The number of nodes that this node pool has. Must be higher than 0",
							ValidateDiagFunc: minimumValue(1, "number of nodes must be higher than 0"),
						},
						"disk_size_gi": {
							Type:             schema.TypeInt,
							Optional:         true,
							Default:          20, // As suggested in UI
							ForceNew:         true,
							Description:      "Disk size, in Gibibytes, for the control plane nodes",
							ValidateDiagFunc: minimumValue(20, "disk size in Gibibytes must be at least 20"),
						},
						"sizing_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Sizing policy for the control plane nodes",
						},
						"placement_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Placement policy for the control plane nodes",
						},
						"vgpu_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "vGPU policy for the control plane nodes",
						},
						"storage_profile_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "Storage profile for the control plane nodes",
						},
					},
				},
			},
			"default_storage_class": {
				Type:     schema.TypeList,
				MaxItems: 1,
				Optional: true,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"storage_profile_id": {
							Required:    true,
							Type:        schema.TypeString,
							Description: "ID of the storage profile to use for the storage class",
						},
						"name": {
							Required:    true,
							Type:        schema.TypeString,
							Description: "Name to give to this storage class",
							ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
								"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
						},
						"reclaim_policy": {
							Required:     true,
							Type:         schema.TypeString,
							ValidateFunc: validation.StringInSlice([]string{"delete", "retain"}, false),
							Description:  "'delete' deletes the volume when the PersistentVolumeClaim is deleted. 'retain' does not, and the volume can be manually reclaimed",
						},
						"filesystem": {
							Required:     true,
							Type:         schema.TypeString,
							ValidateFunc: validation.StringInSlice([]string{"ext4", "xfs"}, false),
							Description:  "Filesystem of the storage class, can be either 'ext4' or 'xfs'",
						},
					},
				},
			},
			"pods_cidr": {
				Type:        schema.TypeString,
				Optional:    true,
				Default:     "100.96.0.0/11", // As suggested in UI
				Description: "CIDR that the Kubernetes pods will use",
			},
			"services_cidr": {
				Type:        schema.TypeString,
				Optional:    true,
				Default:     "100.64.0.0/13", // As suggested in UI
				Description: "CIDR that the Kubernetes services will use",
			},
			"virtual_ip_subnet": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: "Virtual IP subnet for the cluster",
			},
			"auto_repair_on_errors": {
				Type:        schema.TypeBool,
				Optional:    true,
				Default:     false,
				Description: "If errors occur before the Kubernetes cluster becomes available, and this argument is 'true', CSE Server will automatically attempt to repair the cluster",
			},
			"node_health_check": {
				Type:        schema.TypeBool,
				Optional:    true,
				Default:     false,
				Description: "After the Kubernetes cluster becomes available, nodes that become unhealthy will be remediated according to unhealthy node conditions and remediation rules",
			},
			"create_timeout_minutes": {
				Type:             schema.TypeInt,
				Optional:         true,
				Default:          60,
				Description:      "The time, in minutes, to wait for the cluster to be completely created, with a ready-to-use Kubeconfig. 0 means wait indefinitely",
				ValidateDiagFunc: minimumValue(0, "timeout must be at least 0 (no timeout)"),
			},
			"delete_timeout_minutes": {
				Type:             schema.TypeInt,
				Optional:         true,
				Default:          10,
				Description:      "The time, in minutes, to wait for the cluster to be deleted when it is marked for deletion. 0 means wait indefinitely",
				ValidateDiagFunc: minimumValue(0, "timeout must be at least 0 (no timeout)"),
			},
			"state": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The state of the cluster, can be 'provisioning', 'provisioned', 'deleting' or 'error'. Useful to check whether the Kubernetes cluster is in a stable status",
			},
			"kubeconfig": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The contents of the kubeconfig of the Kubernetes cluster, only available when 'state=provisioned'",
			},
			"raw_cluster_rde_json": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The raw JSON that describes the cluster configuration inside the Runtime Defined Entity",
			},
		},
	}
}

func resourceVcdCseKubernetesClusterCreate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)
	clusterDetails, err := getClusterCreateDto(d, vcdClient)
	if err != nil {
		return diag.Errorf("could not create Kubernetes cluster: %s", err)
	}

	entityMap, err := getCseKubernetesClusterEntityMap(d, clusterDetails)
	if err != nil {
		return diag.Errorf("could not create Kubernetes cluster with name '%s': %s", clusterDetails.Name, err)
	}

	rde, err := clusterDetails.RdeType.CreateRde(types.DefinedEntity{
		EntityType: clusterDetails.RdeType.DefinedEntityType.ID,
		Name:       clusterDetails.Name,
		Entity:     entityMap,
	}, &govcd.TenantContext{
		OrgId:   clusterDetails.Org.AdminOrg.ID,
		OrgName: clusterDetails.Org.AdminOrg.Name,
	})
	if err != nil {
		return diag.Errorf("could not create Kubernetes cluster with name '%s': %s", clusterDetails.Name, err)
	}

	// We need to set the ID here to be able to distinguish this cluster from all the others that may have the same name and RDE Type.
	// We could use some other ways of filtering, but ID is the only accurate.
	// Also, the RDE is created at this point, so Terraform should trigger an update/delete next.
	// If the cluster can't be created due to errors, users should delete it and retry, like in UI.
	d.SetId(rde.DefinedEntity.ID)

	state, err := waitUntilClusterIsProvisioned(vcdClient, d, rde.DefinedEntity.ID)
	if err != nil {
		return diag.Errorf("Kubernetes cluster creation failed: %s", err)
	}
	if state != "provisioned" {
		return diag.Errorf("Kubernetes cluster creation failed, cluster is not in 'provisioned' state, but '%s'", state)
	}

	return resourceVcdCseKubernetesRead(ctx, d, meta)
}

// waitUntilClusterIsProvisioned waits for the Kubernetes cluster to be in "provisioned" state, either indefinitely (if "create_timeout_minutes=0")
// or until this timeout is reached. If one of the states is "error", this function also checks whether "auto_repair_on_errors=true" to keep
// waiting.
func waitUntilClusterIsProvisioned(vcdClient *VCDClient, d *schema.ResourceData, rdeId string) (string, error) {
	var elapsed time.Duration
	timeout := d.Get("create_timeout_minutes").(int)
	currentState := ""

	start := time.Now()
	for elapsed <= time.Duration(timeout)*time.Minute || timeout == 0 { // If the user specifies create_timeout_minutes=0, we wait forever
		rde, err := vcdClient.GetRdeById(rdeId)
		if err != nil {
			return "", err
		}
		currentState, err = traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
		if err != nil {
			util.Logger.Printf("[DEBUG] Failed getting cluster state: %s", err)
			// We ignore this error, as eventually the state should be populated
		} else {

			// Add some traceability in the logs and Terraform output about the progress of the cluster provisioning
			eventSet, err := traverseMapAndGet[[]interface{}](rde.DefinedEntity.Entity, "status.vcdKe.eventSet")
			if err == nil {
				latestEvent, err := traverseMapAndGet[string](eventSet[len(eventSet)-1], "additionalDetails.Detailed Event")
				if err != nil {
					util.Logger.Printf("[DEBUG] waiting for cluster to be provisioned. Latest event: '%s'", latestEvent)
				}
			}

			switch currentState {
			case "provisioned":
				return currentState, nil
			case "error":
				// We just finish if auto-recovery is disabled, otherwise we just let CSE fixing things in background
				if !d.Get("auto_repair_on_errors").(bool) {
					// Try to give feedback about what went wrong, which is located in a set of events in the RDE payload
					latestError := "could not parse error event"
					errorSet, err := traverseMapAndGet[[]interface{}](rde.DefinedEntity.Entity, "status.capvcd.errorSet")
					if err == nil {
						latestError, err = traverseMapAndGet[string](errorSet[len(errorSet)-1], "additionalDetails.error")
						if err != nil {
							latestError = "could not parse error event"
						}
					}
					return "", fmt.Errorf("got an error and 'auto_repair_on_errors=false', aborting. Latest error: %s", latestError)
				}
			}
		}

		elapsed = time.Since(start)
		time.Sleep(50 * time.Second)

	}
	return "", fmt.Errorf("timeout of %d minutes reached, latest cluster state obtained was '%s'", timeout, currentState)
}

func resourceVcdCseKubernetesRead(_ context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)
	var diags diag.Diagnostics
	behaviorVersion := supportedCseVersions[d.Get("cse_version").(string)][2]

	// The ID must be already set for the read to be successful. We can't rely on GetRdesByName as there can be
	// many clusters with the same name and RDE Type.
	var err error
	rde, err := vcdClient.GetRdeById(d.Id())
	if err != nil {
		return diag.Errorf("could not read Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}

	state, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
	if err != nil {
		return diag.Errorf("could not read Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	dSet(d, "state", state)

	if state == "provisioned" {
		// This can only be done if the cluster is in 'provisioned' state
		invocationResult := map[string]interface{}{}
		err := rde.InvokeBehaviorAndMarshal(fmt.Sprintf("urn:vcloud:behavior-interface:getFullEntity:cse:capvcd:%s", behaviorVersion), types.BehaviorInvocation{}, invocationResult)
		if err != nil {
			return diag.Errorf("could not invoke the behavior to obtain the Kubeconfig for the Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}

		kubeconfig, err := traverseMapAndGet[string](invocationResult, "entity.status.capvcd.private.kubeConfig")
		if err != nil {
			return diag.Errorf("could not retrieve Kubeconfig for Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}
		dSet(d, "kubeconfig", kubeconfig)
	} else {
		diags = append(diags, diag.Diagnostic{
			Severity: diag.Warning,
			Summary:  "Kubernetes cluster not in 'provisioned' state",
			Detail:   fmt.Sprintf("Kubernetes cluster with ID '%s' is in '%s' state, won't be able to retrieve the Kubeconfig", d.Id(), state),
		})
	}

	jsonEntity, err := jsonToCompactString(rde.DefinedEntity.Entity)
	if err != nil {
		diags = append(diags, diag.Errorf("could not save the cluster '%s' raw RDE contents into 'raw_cluster_rde_json' attribute: %s", rde.DefinedEntity.ID, err)...)
	}
	if diags != nil && diags.HasError() {
		return diags
	}
	dSet(d, "raw_cluster_rde_json", jsonEntity)

	d.SetId(rde.DefinedEntity.ID) // ID is already there, but just for completeness/readability
	return nil
}

func resourceVcdCseKubernetesUpdate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)

	// The ID must be already set for the read to be successful. We can't rely on GetRdesByName as there can be
	// many clusters with the same name and RDE Type.
	rde, err := vcdClient.GetRdeById(d.Id())
	if err != nil {
		return diag.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	state, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
	if err != nil {
		return diag.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	if state != "provisioned" {
		return diag.Errorf("could not update the Kubernetes cluster with ID '%s': It is in '%s' state, but should be 'provisioned'", d.Id(), state)
	}
	// Only OVA and pool sizes can be changed. This is guaranteed by all ForceNew flags, but it's worth it to
	// double-check
	if d.HasChangesExcept("ova_id", "control_plane.0.machine_count", "node_pool") {
		return diag.Errorf("only the Kubernetes template or the control plane/node machine pools can be modified")
	}

	// Gets and unmarshals the CAPI YAML to update it
	capiYaml, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "spec.capiYaml")
	if err != nil {
		return diag.Errorf("could not retrieve the CAPI YAML from the Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	capiMap := map[string]interface{}{}
	err = yaml.Unmarshal([]byte(capiYaml), &capiMap)
	if err != nil {
		return diag.Errorf("could not unmarshal the CAPI YAML from the Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}

	// TODO: Change YAML here
	if d.HasChange("ova_id") {
		newOva := d.Get("ova_id")
		_, err := vcdClient.GetVAppTemplateById(newOva.(string))
		if err != nil {
			return diag.Errorf("could not retrieve the new Kubernetes OVA with ID '%s': %s", newOva, err)
		}
		// TODO: Check whether the update can be performed
	}
	if d.HasChange("control_plane.0.machine_count") {
		util.Logger.Printf("not done but make static complains :)")
	}
	if d.HasChange("node_pool") {
		util.Logger.Printf("not done but make static complains :)")
	}

	updatedYaml := capiYaml // FIXME
	rde.DefinedEntity.Entity["spec"].(map[string]interface{})["capiYaml"] = updatedYaml

	// FIXME: This must be done with retries due to ETag clash
	err = rde.Update(*rde.DefinedEntity)
	if err != nil {
		return diag.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}

	return resourceVcdCseKubernetesRead(ctx, d, meta)
}

// resourceVcdCseKubernetesDelete deletes a CSE Kubernetes cluster. To delete a Kubernetes cluster, one must send
// the flags "markForDelete" and "forceDelete" back to true, so the CSE Server is able to delete all cluster elements
// and perform a cleanup. Hence, this function sends an update of just these two properties and waits for the cluster RDE
// to be gone.
func resourceVcdCseKubernetesDelete(_ context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)
	vcdKe := map[string]interface{}{}

	var elapsed time.Duration
	timeout := d.Get("delete_timeout_minutes").(int)
	start := time.Now()
	for elapsed <= time.Duration(timeout)*time.Minute || timeout == 0 { // If the user specifies delete_timeout_minutes=0, we wait forever
		rde, err := vcdClient.GetRdeById(d.Id())
		if err != nil {
			if govcd.ContainsNotFound(err) {
				return nil // The RDE is gone, so the process is completed and there's nothing more to do
			}
			return diag.Errorf("could not retrieve the Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}

		vcdKe, err = traverseMapAndGet[map[string]interface{}](rde.DefinedEntity.Entity, "spec.vcdKe")
		if err != nil {
			return diag.Errorf("JSON object 'spec.vcdKe' is not correct in the RDE '%s': %s", d.Id(), err)
		}

		if !vcdKe["markForDelete"].(bool) || !vcdKe["forceDelete"].(bool) {
			// Mark the cluster for deletion
			vcdKe["markForDelete"] = true
			vcdKe["forceDelete"] = true
			rde.DefinedEntity.Entity["spec"].(map[string]interface{})["vcdKe"] = vcdKe
			err = rde.Update(*rde.DefinedEntity)
			if err != nil {
				if strings.Contains(strings.ToLower(err.Error()), "etag") {
					continue // We ignore any ETag error. This just means a clash between CSE Server and Terraform, we just try again
				}
				return diag.Errorf("could not mark the Kubernetes cluster with ID '%s' to be deleted: %s", d.Id(), err)
			}
		}

		time.Sleep(30 * time.Second)
		elapsed = time.Since(start)
	}

	// We give a hint to the user whenever possible
	if len(vcdKe) >= 2 && vcdKe["markForDelete"].(bool) && vcdKe["forceDelete"].(bool) {
		return diag.Errorf("timeout of %d minutes reached, the cluster was successfully marked for deletion but was not removed in time", timeout)
	}
	return diag.Errorf("timeout of %d minutes reached, the cluster was not marked for deletion, please try again", timeout)
}

// getCseKubernetesClusterEntityMap gets the payload for the RDE that manages the Kubernetes cluster, so it
// can be created or updated.
func getCseKubernetesClusterEntityMap(d *schema.ResourceData, clusterDetails *createClusterDto) (map[string]interface{}, error) {
	capiYaml, err := generateCapiYaml(d, clusterDetails)
	if err != nil {
		return nil, err
	}

	args := map[string]string{
		"Name":               clusterDetails.Name,
		"Org":                clusterDetails.Org.AdminOrg.Name,
		"VcdUrl":             clusterDetails.VcdUrl,
		"Vdc":                clusterDetails.VdcName,
		"Delete":             "false",
		"ForceDelete":        "false",
		"AutoRepairOnErrors": strconv.FormatBool(d.Get("auto_repair_on_errors").(bool)),
		"ApiToken":           clusterDetails.ApiToken,
		"CapiYaml":           capiYaml,
	}

	if _, isStorageClassSet := d.GetOk("default_storage_class"); isStorageClassSet {
		args["DefaultStorageClassStorageProfile"] = clusterDetails.UrnToNamesCache[d.Get("default_storage_class.0.storage_profile_id").(string)]
		args["DefaultStorageClassName"] = d.Get("default_storage_class.0.name").(string)
		if d.Get("default_storage_class.0.reclaim_policy").(string) == "delete" {
			args["DefaultStorageClassUseDeleteReclaimPolicy"] = "true"
		} else {
			args["DefaultStorageClassUseDeleteReclaimPolicy"] = "false"
		}
		args["DefaultStorageClassFileSystem"] = d.Get("default_storage_class.0.filesystem").(string)
	}

	rdeTmpl, err := getCseTemplateFile(d, "rde")
	if err != nil {
		return nil, err
	}

	capvcdEmpty := template.Must(template.New(clusterDetails.Name).Parse(rdeTmpl))
	buf := &bytes.Buffer{}
	if err := capvcdEmpty.Execute(buf, args); err != nil {
		return nil, fmt.Errorf("could not render the Go template with the CAPVCD JSON: %s", err)
	}

	var result interface{}
	err = json.Unmarshal(buf.Bytes(), &result)
	if err != nil {
		return nil, fmt.Errorf("could not generate a correct CAPVCD JSON: %s", err)
	}

	return result.(map[string]interface{}), nil
}

// generateNodePoolYaml generates YAML blocks corresponding to the Kubernetes node pools.
func generateNodePoolYaml(d *schema.ResourceData, clusterDetails *createClusterDto) (string, error) {
	nodePoolTmpl, err := getCseTemplateFile(d, "capiyaml_nodepool")
	if err != nil {
		return "", err
	}

	nodePoolEmptyTmpl := template.Must(template.New(clusterDetails.Name + "-node-pool").Parse(nodePoolTmpl))
	resultYaml := ""
	buf := &bytes.Buffer{}

	// We can have many node pool blocks, we build a YAML object for each one of them.
	for _, nodePoolRaw := range d.Get("node_pool").(*schema.Set).List() {
		nodePool := nodePoolRaw.(map[string]interface{})
		name := nodePool["name"].(string)

		// Check the correctness of the compute policies in the node pool block
		placementPolicyId := nodePool["placement_policy_id"]
		vpguPolicyId := nodePool["vgpu_policy_id"]
		if placementPolicyId != "" && vpguPolicyId != "" {
			return "", fmt.Errorf("the node pool '%s' should have either a Placement Policy or a vGPU Policy, not both", name)
		}
		if vpguPolicyId != "" {
			placementPolicyId = vpguPolicyId // For convenience, we just use one of the variables as both cannot be set at same time
		}

		if err := nodePoolEmptyTmpl.Execute(buf, map[string]string{
			"ClusterName":             clusterDetails.Name,
			"NodePoolName":            name,
			"TargetNamespace":         clusterDetails.Name + "-ns",
			"Catalog":                 clusterDetails.CatalogName,
			"VAppTemplate":            clusterDetails.OvaName,
			"NodePoolSizingPolicy":    clusterDetails.UrnToNamesCache[nodePool["sizing_policy_id"].(string)],
			"NodePoolPlacementPolicy": clusterDetails.UrnToNamesCache[placementPolicyId.(string)], // Can be either Placement or vGPU
			"NodePoolStorageProfile":  clusterDetails.UrnToNamesCache[nodePool["storage_profile_id"].(string)],
			"NodePoolDiskSize":        fmt.Sprintf("%dGi", nodePool["disk_size_gi"].(int)),
			"NodePoolEnableGpu":       strconv.FormatBool(vpguPolicyId != ""),
			"NodePoolMachineCount":    strconv.Itoa(nodePool["machine_count"].(int)),
			"KubernetesVersion":       clusterDetails.TkgVersion.KubernetesVersion,
		}); err != nil {
			return "", fmt.Errorf("could not generate a correct Node Pool YAML: %s", err)
		}
		resultYaml += fmt.Sprintf("%s\n---\n", buf.String())
		buf.Reset()
	}
	return resultYaml, nil
}

// tkgVersionBundle is a type that contains all the versions of the components of
// a Kubernetes cluster that can be obtained with the vApp Template name, downloaded
// from VMware Customer connect:
// https://customerconnect.vmware.com/downloads/details?downloadGroup=TKG-240&productId=1400
type tkgVersionBundle struct {
	EtcdVersion       string
	CoreDnsVersion    string
	TkgVersion        string
	TkrVersion        string
	KubernetesVersion string
}

// getTkgVersionBundleFromVAppTemplateName returns a tkgVersionBundle with the details of
// all the Kubernetes cluster components versions given a valid vApp Template name, that should
// correspond to a Kubernetes template. If it is not a valid vApp Template, returns an error.
func getTkgVersionBundleFromVAppTemplateName(ovaName string) (tkgVersionBundle, error) {
	versionsMap := map[string]map[string]string{
		"v1.25.7+vmware.2-tkg.1-8a74b9f12e488c54605b3537acb683bc": {
			"tkg":     "v2.2.0",
			"etcd":    "v3.5.6_vmware.9",
			"coreDns": "v1.9.3_vmware.8",
		},
		"v1.27.5+vmware.1-tkg.1-0eb96d2f9f4f705ac87c40633d4b69st": {
			"tkg":     "v2.4.0",
			"etcd":    "v3.5.7_vmware.6",
			"coreDns": "v1.10.1_vmware.7",
		},
		"v1.26.8+vmware.1-tkg.1-b8c57a6c8c98d227f74e7b1a9eef27st": {
			"tkg":     "v2.4.0",
			"etcd":    "v3.5.6_vmware.20",
			"coreDns": "v1.10.1_vmware.7",
		},
		"v1.26.8+vmware.1-tkg.1-0edd4dafbefbdb503f64d5472e500cf8": {
			"tkg":     "v2.3.1",
			"etcd":    "v3.5.6_vmware.20",
			"coreDns": "v1.9.3_vmware.16",
		},
	}

	result := tkgVersionBundle{}

	if strings.Contains(ovaName, "photon") {
		return result, fmt.Errorf("the vApp Template '%s' uses Photon, and it is not supported", ovaName)
	}

	cutPosition := strings.LastIndex(ovaName, "kube-")
	if cutPosition < 0 {
		return result, fmt.Errorf("the vApp Template '%s' is not a Kubernetes template OVA", ovaName)
	}
	parsedOvaName := strings.ReplaceAll(ovaName, ".ova", "")[cutPosition+len("kube-"):]
	if _, ok := versionsMap[parsedOvaName]; !ok {
		return result, fmt.Errorf("the Kubernetes OVA '%s' is not supported", parsedOvaName)
	}

	// The map checking above guarantees that all splits and replaces will work
	result.KubernetesVersion = strings.Split(parsedOvaName, "-")[0]
	result.TkrVersion = strings.ReplaceAll(strings.Split(parsedOvaName, "-")[0], "+", "---") + "-" + strings.Split(parsedOvaName, "-")[1]
	result.TkgVersion = versionsMap[parsedOvaName]["tkg"]
	result.EtcdVersion = versionsMap[parsedOvaName]["etcd"]
	result.CoreDnsVersion = versionsMap[parsedOvaName]["coreDns"]
	return result, nil
}

// createClusterDto is a helper struct that contains all the required elements to successfully create a Kubernetes cluster using CSE.
// This is useful to avoid querying VCD too much, as the Terraform configuration works mostly with IDs, but we require names, among
// other items that we eventually need to retrieve from VCD.
type createClusterDto struct {
	Name            string
	VcdUrl          string
	Org             *govcd.AdminOrg
	VdcName         string
	OvaName         string
	CatalogName     string
	NetworkName     string
	RdeType         *govcd.DefinedEntityType
	UrnToNamesCache map[string]string // Maps unique IDs with their resource names (example: Compute policy ID with its name)
	VCDKEConfig     struct {
		MaxUnhealthyNodesPercentage string
		NodeStartupTimeout          string
		NodeNotReadyTimeout         string
		NodeUnknownTimeout          string
		ContainerRegistryUrl        string
	}
	TkgVersion tkgVersionBundle
	Owner      string
	ApiToken   string
}

// getClusterCreateDto creates and returns a createClusterDto object by obtaining all the required information
// from the Terraform resource data and the target VCD.
func getClusterCreateDto(d *schema.ResourceData, vcdClient *VCDClient) (*createClusterDto, error) {
	result := &createClusterDto{}
	result.UrnToNamesCache = map[string]string{"": ""} // Initialize with a "zero" entry, used when there's no ID set in the Terraform schema

	name := d.Get("name").(string)
	result.Name = name

	org, err := vcdClient.GetAdminOrgFromResource(d)
	if err != nil {
		return nil, fmt.Errorf("could not retrieve the cluster Organization: %s", err)
	}
	result.Org = org

	vdcId := d.Get("vdc_id").(string)
	vdc, err := org.GetVDCById(vdcId, true)
	if err != nil {
		return nil, fmt.Errorf("could not retrieve the VDC with ID '%s': %s", vdcId, err)
	}
	result.VdcName = vdc.Vdc.Name

	vAppTemplateId := d.Get("ova_id").(string)
	vAppTemplate, err := vcdClient.GetVAppTemplateById(vAppTemplateId)
	if err != nil {
		return nil, fmt.Errorf("could not retrieve the Kubernetes OVA with ID '%s': %s", vAppTemplateId, err)
	}
	result.OvaName = vAppTemplate.VAppTemplate.Name

	tkgVersions, err := getTkgVersionBundleFromVAppTemplateName(vAppTemplate.VAppTemplate.Name)
	if err != nil {
		return nil, err
	}
	result.TkgVersion = tkgVersions

	catalogName, err := vAppTemplate.GetCatalogName()
	if err != nil {
		return nil, fmt.Errorf("could not retrieve the CatalogName of the OVA '%s': %s", vAppTemplateId, err)
	}
	result.CatalogName = catalogName

	networkId := d.Get("network_id").(string)
	network, err := vdc.GetOrgVdcNetworkById(networkId, true)
	if err != nil {
		return nil, fmt.Errorf("could not retrieve the Org VDC NetworkName with ID '%s': %s", networkId, err)
	}
	result.NetworkName = network.OrgVDCNetwork.Name

	currentCseVersion := supportedCseVersions[d.Get("cse_version").(string)]
	rdeType, err := vcdClient.GetRdeType("vmware", "capvcdCluster", currentCseVersion[1])
	if err != nil {
		return nil, fmt.Errorf("could not retrieve RDE Type vmware:capvcdCluster:'%s': %s", currentCseVersion[1], err)
	}
	result.RdeType = rdeType

	// Fills the cache map that relates IDs of Storage profiles and Compute policies (the schema uses them to build a
	// healthy Terraform dependency graph) with their corresponding names (the cluster YAML and CSE in general uses names only).
	// Having this map minimizes the amount of queries to VCD, specially when building the set of node pools,
	// as there can be a lot of them.
	for _, configBlockAttr := range []string{"default_storage_class", "control_plane", "node_pool"} {
		if _, ok := d.GetOk(configBlockAttr); !ok {
			continue // Some blocks are optional, this is managed by the schema constraints
		}
		// The node_pool is a Set, but the others are already Lists
		var configBlockAsList []interface{}
		if _, isASet := d.Get(configBlockAttr).(*schema.Set); isASet {
			configBlockAsList = d.Get(configBlockAttr).(*schema.Set).List()
		} else {
			configBlockAsList = d.Get(configBlockAttr).([]interface{})
		}

		// For every existing block/list, we check the inner attributes to retrieve their corresponding object names,
		// like Storage Profile names and Compute Policy names. If the ID is already registered, we skip it.
		for _, configBlockRaw := range configBlockAsList {
			configBlock := configBlockRaw.(map[string]interface{})
			if id, ok := configBlock["storage_profile_id"]; ok {
				if _, alreadyPresent := result.UrnToNamesCache[id.(string)]; !alreadyPresent {
					storageProfile, err := vcdClient.GetStorageProfileById(id.(string))
					if err != nil {
						return nil, fmt.Errorf("could not get Storage Profile with ID '%s': %s", id, err)
					}
					result.UrnToNamesCache[id.(string)] = storageProfile.Name
				}
			}
			// The other sub-attributes are just Compute policies, we treat them the same
			for _, attribute := range []string{"sizing_policy_id", "vgpu_policy_id", "placement_policy_id"} {
				id, ok := configBlock[attribute]
				if !ok {
					continue
				}
				if _, alreadyPresent := result.UrnToNamesCache[id.(string)]; alreadyPresent {
					continue
				}
				computePolicy, err := vcdClient.GetVdcComputePolicyV2ById(id.(string))
				if err != nil {
					return nil, fmt.Errorf("could not get Compute Policy with ID '%s': %s", id, err)
				}
				result.UrnToNamesCache[id.(string)] = computePolicy.VdcComputePolicyV2.Name
			}
		}
	}

	rdes, err := vcdClient.GetRdesByName("vmware", "VCDKEConfig", currentCseVersion[0], "vcdKeConfig")
	if err != nil {
		return nil, fmt.Errorf("could not retrieve VCDKEConfig RDE with version %s: %s", currentCseVersion[0], err)
	}
	if len(rdes) != 1 {
		return nil, fmt.Errorf("expected exactly one VCDKEConfig RDE but got %d", len(rdes))
	}

	// Obtain some required elements from the CSE Server configuration (aka VCDKEConfig), so we don't have
	// to deal with it again.
	type vcdKeConfigType struct {
		Profiles []struct {
			K8Config struct {
				Mhc struct {
					MaxUnhealthyNodes   int `json:"maxUnhealthyNodes:omitempty"`
					NodeStartupTimeout  int `json:"nodeStartupTimeout:omitempty"`
					NodeNotReadyTimeout int `json:"nodeNotReadyTimeout:omitempty"`
					NodeUnknownTimeout  int `json:"nodeUnknownTimeout:omitempty"`
				} `json:"mhc:omitempty"`
			} `json:"K8Config:omitempty"`
			ContainerRegistryUrl string `json:"containerRegistryUrl,omitempty"`
		} `json:"profiles,omitempty"`
	}

	var vcdKeConfig vcdKeConfigType
	rawData, err := json.Marshal(rdes[0].DefinedEntity.Entity)
	if err != nil {
		return nil, err
	}

	err = json.Unmarshal(rawData, &vcdKeConfig)
	if err != nil {
		return nil, err
	}

	if len(vcdKeConfig.Profiles) != 1 {
		return nil, fmt.Errorf("wrong format of VCDKEConfig, expected a single 'profiles' element, got %d", len(vcdKeConfig.Profiles))
	}

	result.VCDKEConfig.MaxUnhealthyNodesPercentage = strconv.Itoa(vcdKeConfig.Profiles[0].K8Config.Mhc.MaxUnhealthyNodes)
	result.VCDKEConfig.NodeStartupTimeout = strconv.Itoa(vcdKeConfig.Profiles[0].K8Config.Mhc.NodeStartupTimeout)
	result.VCDKEConfig.NodeNotReadyTimeout = strconv.Itoa(vcdKeConfig.Profiles[0].K8Config.Mhc.NodeNotReadyTimeout)
	result.VCDKEConfig.NodeUnknownTimeout = strconv.Itoa(vcdKeConfig.Profiles[0].K8Config.Mhc.NodeUnknownTimeout)
	result.VCDKEConfig.ContainerRegistryUrl = fmt.Sprintf("%s/tkg", vcdKeConfig.Profiles[0].ContainerRegistryUrl)

	owner, ok := d.GetOk("owner")
	if !ok {
		sessionInfo, err := vcdClient.Client.GetSessionInfo()
		if err != nil {
			return nil, fmt.Errorf("error getting the owner of the cluster: %s", err)
		}
		owner = sessionInfo.User.Name
	}
	result.Owner = owner.(string)

	apiToken, err := govcd.GetTokenFromFile(d.Get("api_token_file").(string))
	if err != nil {
		return nil, fmt.Errorf("API token file could not be parsed or found: %s\nPlease check that the format is the one that 'vcd_api_token' resource uses", err)
	}
	result.ApiToken = apiToken.RefreshToken

	result.VcdUrl = strings.Replace(vcdClient.VCDClient.Client.VCDHREF.String(), "/api", "", 1)
	return result, nil
}

// generateCapiYaml generates the YAML string that is required during Kubernetes cluster creation, to be embedded
// in the CAPVCD cluster JSON payload. This function picks data from the Terraform schema and the createClusterDto to
// populate several Go templates and build a final YAML.
func generateCapiYaml(d *schema.ResourceData, clusterDetails *createClusterDto) (string, error) {
	clusterTmpl, err := getCseTemplateFile(d, "capiyaml_cluster")
	if err != nil {
		return "", err
	}

	// This YAML snippet contains special strings, such as "%,", that render wrong using the Go template engine
	sanitizedTemplate := strings.NewReplacer("%", "%%").Replace(clusterTmpl)
	capiYamlEmpty := template.Must(template.New(clusterDetails.Name + "-cluster").Parse(sanitizedTemplate))

	nodePoolYaml, err := generateNodePoolYaml(d, clusterDetails)
	if err != nil {
		return "", err
	}

	args := map[string]string{
		"ClusterName":                 clusterDetails.Name,
		"TargetNamespace":             clusterDetails.Name + "-ns",
		"TkrVersion":                  clusterDetails.TkgVersion.TkrVersion,
		"TkgVersion":                  clusterDetails.TkgVersion.TkgVersion,
		"UsernameB64":                 base64.StdEncoding.EncodeToString([]byte(clusterDetails.Owner)),
		"ApiTokenB64":                 base64.StdEncoding.EncodeToString([]byte(clusterDetails.ApiToken)),
		"PodCidr":                     d.Get("pods_cidr").(string),
		"ServiceCidr":                 d.Get("services_cidr").(string),
		"VcdSite":                     clusterDetails.VcdUrl,
		"Org":                         clusterDetails.Org.AdminOrg.Name,
		"OrgVdc":                      clusterDetails.VdcName,
		"OrgVdcNetwork":               clusterDetails.NetworkName,
		"Catalog":                     clusterDetails.CatalogName,
		"VAppTemplate":                clusterDetails.OvaName,
		"ControlPlaneSizingPolicy":    clusterDetails.UrnToNamesCache[d.Get("control_plane.0.sizing_policy_id").(string)],
		"ControlPlanePlacementPolicy": clusterDetails.UrnToNamesCache[d.Get("control_plane.0.placement_policy_id").(string)],
		"ControlPlaneStorageProfile":  clusterDetails.UrnToNamesCache[d.Get("control_plane.0.storage_profile_id").(string)],
		"ControlPlaneDiskSize":        fmt.Sprintf("%dGi", d.Get("control_plane.0.disk_size_gi").(int)),
		"ControlPlaneMachineCount":    strconv.Itoa(d.Get("control_plane.0.machine_count").(int)),
		"DnsVersion":                  clusterDetails.TkgVersion.CoreDnsVersion,
		"EtcdVersion":                 clusterDetails.TkgVersion.EtcdVersion,
		"ContainerRegistryUrl":        clusterDetails.VCDKEConfig.ContainerRegistryUrl,
		"KubernetesVersion":           clusterDetails.TkgVersion.KubernetesVersion,
		"SshPublicKey":                d.Get("ssh_public_key").(string),
	}
	if _, ok := d.GetOk("control_plane.0.ip"); ok {
		args["ControlPlaneEndpoint"] = d.Get("control_plane.0.ip").(string)
	}
	if _, ok := d.GetOk("virtual_ip_subnet"); ok {
		args["VirtualIpSubnet"] = d.Get("virtual_ip_subnet").(string)
	}
	if d.Get("node_health_check").(bool) {
		args["MaxUnhealthyNodePercentage"] = fmt.Sprintf("%s%%%%", clusterDetails.VCDKEConfig.MaxUnhealthyNodesPercentage) // With the 'percentage' suffix, it is doubled to render the template correctly
		args["NodeStartupTimeout"] = fmt.Sprintf("%ss", clusterDetails.VCDKEConfig.NodeStartupTimeout)                     // With the 'second' suffix
		args["NodeUnknownTimeout"] = fmt.Sprintf("%ss", clusterDetails.VCDKEConfig.NodeUnknownTimeout)                     // With the 'second' suffix
		args["NodeNotReadyTimeout"] = fmt.Sprintf("%ss", clusterDetails.VCDKEConfig.NodeNotReadyTimeout)                   // With the 'second' suffix
	}

	buf := &bytes.Buffer{}
	if err := capiYamlEmpty.Execute(buf, args); err != nil {
		return "", fmt.Errorf("could not generate a correct CAPI YAML: %s", err)
	}
	// The final "pretty" YAML. To embed it in the final payload it must be marshaled into a one-line JSON string
	prettyYaml := fmt.Sprintf("%s\n%s", nodePoolYaml, buf.String())

	// This encoder is used instead of a standard json.Marshal as the YAML contains special
	// characters that are not encoded properly, such as '<'.
	buf.Reset()
	enc := json.NewEncoder(buf)
	enc.SetEscapeHTML(false)
	err = enc.Encode(prettyYaml)
	if err != nil {
		return "", fmt.Errorf("could not encode the CAPI YAML into JSON: %s", err)
	}

	// Removes trailing quotes from the final JSON string
	return strings.Trim(strings.TrimSpace(buf.String()), "\""), nil
}

// getCseTemplateFile gets a Go template file corresponding to the CSE version set in the Terraform configuration
func getCseTemplateFile(d *schema.ResourceData, templateName string) (string, error) {
	cseVersion := d.Get("cse_version").(string)

	// In the future, we can put here some logic for equivalent CSE versions, to avoid duplicating the same Go
	// templates that didn't change among versions.

	t := fmt.Sprintf("cse/%s/%s.tmpl", cseVersion, templateName)
	b, err := os.ReadFile(filepath.Clean(t))
	if err != nil {
		return "", fmt.Errorf("error reading '%s': %s", t, err)
	}
	return string(b), nil
}
