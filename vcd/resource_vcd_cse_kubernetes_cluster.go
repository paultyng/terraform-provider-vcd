package vcd

import (
	"bytes"
	"context"
	_ "embed"
	"fmt"
	"github.com/hashicorp/go-cty/cty"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"github.com/vmware/go-vcloud-director/v2/govcd"
	"github.com/vmware/go-vcloud-director/v2/types/v56"
	"github.com/vmware/go-vcloud-director/v2/util"
	"gopkg.in/yaml.v3"
	"net/url"
	"strings"
	"time"
)

// supportedCseVersions is a map that contains only the supported CSE versions as keys,
// and its corresponding components versions as a slice of strings. The first string is the VCDKEConfig RDE Type version,
// then the CAPVCD RDE Type version and finally the CAPVCD Behavior version.
// TODO: Is this really necessary? What happens in UI if I have a 1.1.0-1.2.0-1.0.0 (4.2) cluster and then CSE is updated to 4.3?
var supportedCseVersions = map[string][]string{
	"4.2": {
		"1.1.0", // VCDKEConfig RDE Type version
		"1.2.0", // CAPVCD RDE Type version
		"1.0.0", // CAPVCD Behavior version
	},
}

func resourceVcdCseKubernetesCluster() *schema.Resource {
	return &schema.Resource{
		CreateContext: resourceVcdCseKubernetesClusterCreate,
		ReadContext:   resourceVcdCseKubernetesRead,
		UpdateContext: resourceVcdCseKubernetesUpdate,
		DeleteContext: resourceVcdCseKubernetesDelete,
		Importer: &schema.ResourceImporter{
			StateContext: resourceVcdCseKubernetesImport,
		},
		Schema: map[string]*schema.Schema{
			"cse_version": {
				Type:         schema.TypeString,
				Optional:     true, // Required, but validated at runtime
				ForceNew:     true,
				ValidateFunc: validation.StringInSlice(getKeys(supportedCseVersions), false),
				Description:  "The CSE version to use",
			},
			"runtime": {
				Type:         schema.TypeString,
				Optional:     true,
				Default:      "tkg",
				ForceNew:     true,
				ValidateFunc: validation.StringInSlice([]string{"tkg"}, false), // May add others in future releases of CSE
				Description:  "The Kubernetes runtime for the cluster. Only 'tkg' (Tanzu Kubernetes Grid) is supported",
			},
			"name": {
				Type:        schema.TypeString,
				Optional:    true, // Required, but validated at runtime
				ForceNew:    true,
				Description: "The name of the Kubernetes cluster",
				ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
					"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
			},
			"ova_id": {
				Type:        schema.TypeString,
				Optional:    true, // Required, but validated at runtime
				ForceNew:    true,
				Description: "The ID of the vApp Template that corresponds to a Kubernetes template OVA",
			},
			"org": {
				Type:     schema.TypeString,
				Optional: true,
				ForceNew: true,
				Description: "The name of organization that will own this Kubernetes cluster, optional if defined at provider " +
					"level. Useful when connected as sysadmin working across different organizations",
			},
			"vdc_id": {
				Type:        schema.TypeString,
				Optional:    true, // Required, but validated at runtime
				ForceNew:    true,
				Description: "The ID of the VDC that hosts the Kubernetes cluster",
			},
			"network_id": {
				Type:        schema.TypeString,
				Optional:    true, // Required, but validated at runtime
				ForceNew:    true,
				Description: "The ID of the network that the Kubernetes cluster will use",
			},
			"owner": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: "The user that creates the cluster and owns the API token specified in 'api_token'. It must have the 'Kubernetes Cluster Author' role. If not specified, it assumes it's the user from the provider configuration",
			},
			"api_token_file": {
				Type:        schema.TypeString,
				Optional:    true, // Required, but validated at runtime
				ForceNew:    true,
				Description: "A file generated by 'vcd_api_token' resource, that stores the API token used to create and manage the cluster, owned by the user specified in 'owner'. Be careful about this file, as it contains sensitive information",
			},
			"ssh_public_key": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: "The SSH public key used to login into the cluster nodes",
			},
			"control_plane": {
				Type:        schema.TypeList,
				MaxItems:    1,
				Optional:    true, // Required, but validated at runtime
				Description: "Defines the control plane for the cluster",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"machine_count": {
							Type:        schema.TypeInt,
							Optional:    true,
							Default:     3, // As suggested in UI
							Description: "The number of nodes that the control plane has. Must be an odd number and higher than 0",
							ValidateDiagFunc: func(v interface{}, path cty.Path) diag.Diagnostics {
								value, ok := v.(int)
								if !ok {
									return diag.Errorf("could not parse int value '%v' for control plane nodes", v)
								}
								if value < 1 || value%2 == 0 {
									return diag.Errorf("number of control plane nodes must be odd and higher than 0, but it was '%d'", value)
								}
								return nil
							},
						},
						"disk_size_gi": {
							Type:             schema.TypeInt,
							Optional:         true,
							Default:          20, // As suggested in UI
							ForceNew:         true,
							ValidateDiagFunc: minimumValue(20, "disk size in Gibibytes (Gi) must be at least 20"),
							Description:      "Disk size, in Gibibytes (Gi), for the control plane nodes. Must be at least 20",
						},
						"sizing_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Sizing policy for the control plane nodes",
						},
						"placement_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Placement policy for the control plane nodes",
						},
						"storage_profile_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "Storage profile for the control plane nodes",
						},
						"ip": {
							Type:         schema.TypeString,
							Optional:     true,
							Computed:     true,
							ForceNew:     true,
							Description:  "IP for the control plane. It will be automatically assigned during cluster creation if left empty",
							ValidateFunc: checkEmptyOrSingleIP(),
						},
					},
				},
			},
			"node_pool": {
				Type:        schema.TypeList,
				Optional:    true, // Required, but validated at runtime
				Description: "Defines a node pool for the cluster",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"name": {
							Type:        schema.TypeString,
							Optional:    true, // Required, but validated at runtime
							ForceNew:    true,
							Description: "The name of this node pool",
							ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
								"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
						},
						"machine_count": {
							Type:             schema.TypeInt,
							Optional:         true,
							Default:          1, // As suggested in UI
							Description:      "The number of nodes that this node pool has. Must be higher than 0",
							ValidateDiagFunc: minimumValue(0, "number of nodes must be higher than or equal to 0"),
						},
						"disk_size_gi": {
							Type:             schema.TypeInt,
							Optional:         true,
							Default:          20, // As suggested in UI
							ForceNew:         true,
							Description:      "Disk size, in Gibibytes (Gi), for the control plane nodes",
							ValidateDiagFunc: minimumValue(20, "disk size in Gibibytes (Gi) must be at least 20"),
						},
						"sizing_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Sizing policy for the control plane nodes",
						},
						"placement_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Placement policy for the control plane nodes",
						},
						"vgpu_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "vGPU policy for the control plane nodes",
						},
						"storage_profile_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "Storage profile for the control plane nodes",
						},
					},
				},
			},
			"default_storage_class": {
				Type:        schema.TypeList,
				MaxItems:    1,
				Optional:    true,
				Description: "Defines the default storage class for the cluster",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"storage_profile_id": {
							Optional:    true, // Required, but validated at runtime
							ForceNew:    true,
							Type:        schema.TypeString,
							Description: "ID of the storage profile to use for the storage class",
						},
						"name": {
							Optional:    true, // Required, but validated at runtime
							ForceNew:    true,
							Type:        schema.TypeString,
							Description: "Name to give to this storage class",
							ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
								"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
						},
						"reclaim_policy": {
							Optional:     true, // Required, but validated at runtime
							ForceNew:     true,
							Type:         schema.TypeString,
							ValidateFunc: validation.StringInSlice([]string{"delete", "retain"}, false),
							Description:  "'delete' deletes the volume when the PersistentVolumeClaim is deleted. 'retain' does not, and the volume can be manually reclaimed",
						},
						"filesystem": {
							Optional:     true, // Required, but validated at runtime
							ForceNew:     true,
							Type:         schema.TypeString,
							ValidateFunc: validation.StringInSlice([]string{"ext4", "xfs"}, false),
							Description:  "Filesystem of the storage class, can be either 'ext4' or 'xfs'",
						},
					},
				},
			},
			"pods_cidr": {
				Type:        schema.TypeString,
				Optional:    true,
				Default:     "100.96.0.0/11", // As suggested in UI
				Description: "CIDR that the Kubernetes pods will use",
			},
			"services_cidr": {
				Type:        schema.TypeString,
				Optional:    true,
				Default:     "100.64.0.0/13", // As suggested in UI
				Description: "CIDR that the Kubernetes services will use",
			},
			"virtual_ip_subnet": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: "Virtual IP subnet for the cluster",
			},
			"auto_repair_on_errors": {
				Type:        schema.TypeBool,
				Optional:    true,
				Default:     false,
				Description: "If errors occur before the Kubernetes cluster becomes available, and this argument is 'true', CSE Server will automatically attempt to repair the cluster",
			},
			"node_health_check": {
				Type:        schema.TypeBool,
				Optional:    true,
				Default:     false,
				Description: "After the Kubernetes cluster becomes available, nodes that become unhealthy will be remediated according to unhealthy node conditions and remediation rules",
			},
			"operations_timeout_minutes": {
				Type:     schema.TypeInt,
				Optional: true,
				Default:  60,
				Description: "The time, in minutes, to wait for the cluster operations to be successfully completed. For example, during cluster creation/update, it should be in `provisioned`" +
					"state before the timeout is reached, otherwise the operation will return an error. For cluster deletion, this timeout" +
					"specifies the time to wait until the cluster is completely deleted. Setting this argument to `0` means to wait indefinitely",
				ValidateDiagFunc: minimumValue(0, "timeout must be at least 0 (no timeout)"),
			},
			"kubernetes_version": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The version of Kubernetes installed in this cluster",
			},
			"tkg_product_version": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The version of TKG installed in this cluster",
			},
			"capvcd_version": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The version of CAPVCD used by this cluster",
			},
			"cluster_resource_set_bindings": {
				Type:        schema.TypeSet,
				Computed:    true,
				Description: "The cluster resource set bindings of this cluster",
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"cpi_version": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The version of the Cloud Provider Interface used by this cluster",
			},
			"csi_version": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The version of the Container Storage Interface used by this cluster",
			},
			"state": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The state of the cluster, can be 'provisioning', 'provisioned', 'deleting' or 'error'. Useful to check whether the Kubernetes cluster is in a stable status",
			},
			"kubeconfig": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The contents of the kubeconfig of the Kubernetes cluster, only available when 'state=provisioned'",
			},
			"persistent_volumes": {
				Type:        schema.TypeSet,
				Computed:    true,
				Description: "A set of persistent volumes that are present in the cluster, only available when a 'default_storage_class' was provided during cluster creation",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"name": {
							Computed:    true,
							Type:        schema.TypeString,
							Description: "The name of the persistent volume",
						},
						"status": {
							Computed:    true,
							Type:        schema.TypeString,
							Description: "The status of the persistent volume",
						},
						"shared": {
							Computed:    true,
							Type:        schema.TypeString,
							Description: "Whether the persistent volume is shared or not",
						},
						"attached_node_count": {
							Computed:    true,
							Type:        schema.TypeInt,
							Description: "How many nodes are consuming the persistent volume",
						},
						"iops": {
							Computed:    true,
							Type:        schema.TypeInt,
							Description: "I/O operations per second for the persistent volume",
						},
						"size": {
							Computed:    true,
							Type:        schema.TypeInt,
							Description: "Size of the persistent volume",
						},
						"storage_profile": {
							Computed:    true,
							Type:        schema.TypeString,
							Description: "Storage profile name of the persistent volume",
						},
						"owner": {
							Computed:    true,
							Type:        schema.TypeString,
							Description: "Owner of the persistent volume",
						},
					},
				},
			},
		},
	}
}

// validateCseKubernetesClusterSchema validates all the required arguments at runtime. All the required arguments
// are marked as Optional in the schema to facilitate the Import operation, but some of them are actually mandatory.
func validateCseKubernetesClusterSchema(d *schema.ResourceData) diag.Diagnostics {
	var diags diag.Diagnostics
	for _, arg := range []string{"cse_version", "name", "ova_id", "vdc_id", "network_id", "api_token_file", "control_plane", "node_pool"} {
		if _, ok := d.GetOk(arg); !ok {
			diags = append(diags, diag.Errorf("the argument '%s' is required, but no definition was found", arg)...)
		}
	}
	nodePoolsRaw := d.Get("node_pool").([]interface{})
	for _, nodePoolRaw := range nodePoolsRaw {
		nodePool := nodePoolRaw.(map[string]interface{})
		for _, arg := range []string{"name"} {
			if _, ok := nodePool[arg]; !ok {
				diags = append(diags, diag.Errorf("the argument 'node_pool.%s' is required, but no definition was found", arg)...)
			}
		}
	}
	if _, ok := d.GetOk("default_storage_class"); ok {
		for _, arg := range []string{"storage_profile_id", "name", "reclaim_policy", "filesystem"} {
			if _, ok := d.GetOk("default_storage_class.0." + arg); !ok {
				diags = append(diags, diag.Errorf("the argument 'default_storage_class.%s' is required, but no definition was found", arg)...)
			}
		}
	}

	if len(diags) > 0 {
		return diags
	}
	return nil
}

func resourceVcdCseKubernetesClusterCreate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	diags := validateCseKubernetesClusterSchema(d)
	if diags != nil {
		return diags
	}

	vcdClient := meta.(*VCDClient)
	org, err := vcdClient.GetOrgFromResource(d)
	if err != nil {
		return diag.Errorf("could not create a Kubernetes cluster in the target Organization: %s", err)
	}

	creationData := govcd.CseClusterCreationInput{
		Name:                    d.Get("name").(string),
		OrganizationId:          org.Org.ID,
		VdcId:                   d.Get("vdc_id").(string),
		NetworkId:               d.Get("network_id").(string),
		KubernetesTemplateOvaId: d.Get("ova_id").(string),
		CseVersion:              d.Get("cse_version").(string),
		ControlPlane: govcd.ControlPlaneInput{
			MachineCount:      d.Get("control_plane.0.machine_count").(int),
			DiskSizeGi:        d.Get("control_plane.0.disk_size_gi").(int),
			SizingPolicyId:    d.Get("control_plane.0.sizing_policy_id").(string),
			PlacementPolicyId: d.Get("control_plane.0.placement_policy_id").(string),
			StorageProfileId:  d.Get("control_plane.0.storage_profile_id").(string),
			Ip:                d.Get("control_plane.0.ip").(string),
		},
	}

	workerPoolsAttr := d.Get("worker_pool").(*schema.Set).List()
	workerPools := make([]govcd.WorkerPoolInput, len(workerPoolsAttr))
	for i, workerPoolRaw := range workerPoolsAttr {
		workerPool := workerPoolRaw.(map[string]interface{})
		workerPools[i] = govcd.WorkerPoolInput{
			Name:              workerPool["name"].(string),
			MachineCount:      workerPool["machine_count"].(int),
			DiskSizeGi:        workerPool["disk_size_gi"].(int),
			SizingPolicyId:    workerPool["sizing_policy_id"].(string),
			PlacementPolicyId: workerPool["placement_policy_id"].(string),
			VGpuPolicyId:      workerPool["vgpu_policy_id"].(string),
			StorageProfileId:  workerPool["storage_profile_id"].(string),
		}
	}
	creationData.WorkerPools = workerPools

	if _, ok := d.GetOk("default_storage_class"); ok {
		creationData.DefaultStorageClass = &govcd.DefaultStorageClassInput{
			StorageProfileId: d.Get("default_storage_class.0.storage_profile_id").(string),
			Name:             d.Get("default_storage_class.0.name").(string),
			ReclaimPolicy:    d.Get("default_storage_class.0.reclaim_policy").(string),
			Filesystem:       d.Get("default_storage_class.0.filesystem").(string),
		}
	}

	cluster, err := vcdClient.CseCreateKubernetesCluster(creationData, time.Duration(d.Get("operations_timeout_minutes").(int))*time.Minute)
	if err != nil {
		if cluster != nil {
			if cluster.Capvcd.Status.VcdKe.State != "provisioned" {
				return diag.Errorf("Kubernetes cluster creation finished, but it is in '%s' state, not 'provisioned': '%s'", cluster.Capvcd.Status.VcdKe.State, err)
			}
		}
		return diag.Errorf("Kubernetes cluster creation failed: %s", err)
	}
	// We need to set the ID here to be able to distinguish this cluster from all the others that may have the same name and RDE Type.
	// We could use some other ways of filtering, but ID is the only accurate.
	// Also, the RDE is created at this point, so Terraform should trigger an update/delete next.
	// If the cluster can't be created due to errors, users should delete it and retry, like in UI.
	d.SetId(cluster.ID)

	return resourceVcdCseKubernetesRead(ctx, d, meta)
}

func resourceVcdCseKubernetesRead(_ context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)
	var diags diag.Diagnostics

	// The ID must be already set for the read to be successful. We can't rely on GetRdesByName as there can be
	// many clusters with the same name and RDE Type.
	var err error
	cluster, err := vcdClient.GetKubernetesClusterById(d.Id())
	if err != nil {
		return diag.Errorf("could not read Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}

	warns, err := saveClusterDataToState(d, vcdClient, cluster, d.Get("cse_version").(string))
	if err != nil {
		return diag.Errorf("could not save Kubernetes cluster data into Terraform state: %s", err)
	}
	for _, warning := range warns {
		diags = append(diags, diag.Diagnostic{
			Severity: diag.Warning,
			Summary:  warning.Error(),
		})
	}

	if len(diags) > 0 {
		return diags
	}
	return nil
}

// resourceVcdCseKubernetesUpdate updates the Kubernetes clusters. Note that re-creating the CAPI YAML and sending it
// back will break everything, so we must patch the YAML piece by piece.
func resourceVcdCseKubernetesUpdate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	diags := validateCseKubernetesClusterSchema(d)
	if diags != nil {
		return diags
	}

	// Some arguments don't require changes in the backend
	if !d.HasChangesExcept("operations_timeout_minutes") {
		return nil
	}

	vcdClient := meta.(*VCDClient)

	// The ID must be already set for the update to be successful. We can't rely on GetRdesByName as there can be
	// many clusters with the same name and RDE Type.
	rde, err := vcdClient.GetRdeById(d.Id())
	if err != nil {
		return diag.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	state, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
	if err != nil {
		return diag.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	if state != "provisioned" {
		return diag.Errorf("could not update the Kubernetes cluster with ID '%s': It is in '%s' state, but should be 'provisioned'", d.Id(), state)
	}

	// Gets and unmarshals the CAPI YAML to update it
	capiYaml, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "spec.capiYaml")
	if err != nil {
		return diag.Errorf("could not retrieve the CAPI YAML from the Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	// TODO: Is there a simpler way?
	dec := yaml.NewDecoder(bytes.NewReader([]byte(capiYaml)))
	var yamlDocs []map[string]interface{}
	i := 0
	for {
		yamlDocs = append(yamlDocs, map[string]interface{}{})
		if dec.Decode(&yamlDocs[i]) != nil {
			break
		}
		i++
	}

	if d.HasChange("ova_id") {
		newOva := d.Get("ova_id")
		ova, err := vcdClient.GetVAppTemplateById(newOva.(string))
		if err != nil {
			return diag.Errorf("could not retrieve the new Kubernetes OVA with ID '%s': %s", newOva, err)
		}
		// TODO: Check whether the update can be performed
		for _, yamlDoc := range yamlDocs {
			if yamlDoc["kind"] == "VCDMachineTemplate" {
				yamlDoc["spec"].(map[string]interface{})["template"].(map[string]interface{})["spec"].(map[string]interface{})["template"] = ova.VAppTemplate.Name
			}
		}
	}
	if d.HasChange("control_plane.0.machine_count") {
		for _, yamlDoc := range yamlDocs {
			if yamlDoc["kind"] == "KubeadmControlPlane" {
				yamlDoc["spec"].(map[string]interface{})["replicas"] = d.Get("control_plane.0.machine_count")
			}
		}
	}
	// The node pools can only be created and resized
	var newNodePools []map[string]interface{}
	if d.HasChange("node_pool") {
		for _, nodePoolRaw := range d.Get("node_pool").(*schema.Set).List() {
			nodePool := nodePoolRaw.(map[string]interface{})
			for _, yamlDoc := range yamlDocs {
				if yamlDoc["kind"] == "MachineDeployment" {
					if yamlDoc["metadata"].(map[string]interface{})["name"] == nodePool["name"].(string) {
						yamlDoc["spec"].(map[string]interface{})["replicas"] = nodePool["machine_count"].(int)
					} else {
						// TODO: Create node pool
						newNodePools = append(newNodePools, map[string]interface{}{})
					}
				}
			}
		}
	}
	if len(newNodePools) > 0 {
		yamlDocs = append(yamlDocs, newNodePools...)
	}

	if d.HasChange("node_health_check") {
		oldNhc, newNhc := d.GetChange("node_health_check")
		if oldNhc.(bool) && !newNhc.(bool) {
			toDelete := 0
			for i, yamlDoc := range yamlDocs {
				if yamlDoc["kind"] == "MachineHealthCheck" {
					toDelete = i
				}
			}
			yamlDocs[toDelete] = yamlDocs[len(yamlDocs)-1] // We delete the MachineHealthCheck block by putting the last doc in its place
			yamlDocs = yamlDocs[:len(yamlDocs)-1]          // Then we remove the last doc
		} else {
			// Add the YAML block
			vcdKeConfig, err := getVcdKeConfiguration(d, vcdClient)
			if err != nil {
				return diag.FromErr(err)
			}
			rawYaml, err := generateMemoryHealthCheckYaml(d, vcdClient, *vcdKeConfig, d.Get("name").(string))
			if err != nil {
				return diag.FromErr(err)
			}
			yamlBlock := map[string]interface{}{}
			err = yaml.Unmarshal([]byte(rawYaml), &yamlBlock)
			if err != nil {
				return diag.Errorf("error updating Memory Health Check: %s", err)
			}
			yamlDocs = append(yamlDocs, yamlBlock)
		}
		util.Logger.Printf("not done but make static complains :)")
	}

	updatedYaml, err := yaml.Marshal(yamlDocs)
	if err != nil {
		return diag.Errorf("error updating cluster: %s", err)
	}

	// This must be done with retries due to the possible clash on ETags
	_, err = runWithRetry(
		"update cluster",
		"could not update cluster",
		1*time.Minute,
		nil,
		func() (any, error) {
			rde, err := vcdClient.GetRdeById(d.Id())
			if err != nil {
				return nil, fmt.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
			}

			rde.DefinedEntity.Entity["spec"].(map[string]interface{})["capiYaml"] = updatedYaml
			rde.DefinedEntity.Entity["spec"].(map[string]interface{})["vcdKe"].(map[string]interface{})["autoRepairOnErrors"] = d.Get("auto_repair_on_errors").(bool)

			//	err = rde.Update(*rde.DefinedEntity)
			util.Logger.Printf("ADAM: PERFORM UPDATE: %v", rde.DefinedEntity.Entity)
			if err != nil {
				return nil, err
			}
			return nil, nil
		},
	)
	if err != nil {
		return diag.FromErr(err)
	}

	state, err = waitUntilClusterIsProvisioned(vcdClient, d, rde.DefinedEntity.ID)
	if err != nil {
		return diag.Errorf("Kubernetes cluster update failed: %s", err)
	}
	if state != "provisioned" {
		return diag.Errorf("Kubernetes cluster update failed, cluster is not in 'provisioned' state, but '%s'", state)
	}

	return resourceVcdCseKubernetesRead(ctx, d, meta)
}

// resourceVcdCseKubernetesDelete deletes a CSE Kubernetes cluster. To delete a Kubernetes cluster, one must send
// the flags "markForDelete" and "forceDelete" back to true, so the CSE Server is able to delete all cluster elements
// and perform a cleanup. Hence, this function sends an update of just these two properties and waits for the cluster RDE
// to be gone.
func resourceVcdCseKubernetesDelete(_ context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)

	cluster, err := vcdClient.GetKubernetesClusterById(d.Id())
	if err != nil {
		return diag.FromErr(err)
	}
	err = cluster.Delete(time.Duration(d.Get("operations_timeout_minutes").(int)))
	if err != nil {
		return diag.FromErr(err)
	}
}

func resourceVcdCseKubernetesImport(_ context.Context, d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	vcdClient := meta.(*VCDClient)

	resourceURI := strings.Split(d.Id(), ImportSeparator)
	var rdeId, cseVersion string
	switch len(resourceURI) {
	case 2: // ImportSeparator != '.'
		cseVersion = resourceURI[0]
		rdeId = resourceURI[1]
	case 3: // ImportSeparator == '.'
		cseVersion = fmt.Sprintf("%s.%s", resourceURI[0], resourceURI[1])
		rdeId = resourceURI[2]
	default:
		return nil, fmt.Errorf("resource name must be specified as cse_version.cluster_id")
	}
	dSet(d, "cse_version", cseVersion)

	rde, err := vcdClient.GetRdeById(rdeId)
	if err != nil {
		return nil, fmt.Errorf("error retrieving Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}

	warns, err := saveClusterDataToState(d, vcdClient, rde, cseVersion)
	if err != nil {
		return nil, fmt.Errorf("failed importing Kubernetes cluster '%s': %s", rdeId, err)
	}
	for _, warn := range warns {
		// We can't do much here as Import does not support Diagnostics
		logForScreen(rdeId, fmt.Sprintf("got a warning during import: %s", warn))
	}

	return []*schema.ResourceData{d}, nil
}

// saveClusterDataToState reads the received RDE contents and sets the Terraform arguments and attributes.
// Returns a slice of warnings first and an error second.
func saveClusterDataToState(d *schema.ResourceData, vcdClient *VCDClient, cluster *govcd.CseClusterApiProviderCluster, cseVersion string) ([]error, error) {
	var warnings []error

	d.SetId(cluster.ID)
	dSet(d, "name", cluster.Capvcd.Name)
	dSet(d, "cse_version", cseVersion)
	dSet(d, "runtime", "tkg") // Only one supported

	// TODO CSE: Why is this a slice???
	if len(cluster.Capvcd.Status.Capvcd.VcdProperties.Organizations) == 0 {
		return nil, fmt.Errorf("expected at least one Organization in cluster '%s'", d.Id())
	}

	// This field is optional, as it can take the value from the VCD client
	if _, ok := d.GetOk("org"); ok {
		dSet(d, "org", cluster.Capvcd.Status.Capvcd.VcdProperties.Organizations[0].Name)
	}
	adminOrg, err := vcdClient.GetAdminOrgByName(cluster.Capvcd.Status.Capvcd.VcdProperties.Organizations[0].Name)
	if err != nil {
		return nil, fmt.Errorf("could not get Organization with name %s: %s", cluster.Capvcd.Status.Capvcd.VcdProperties.Organizations[0].Name, err)
	}

	// TODO CSE: Why is this a slice???
	if len(cluster.Capvcd.Status.Capvcd.VcdProperties.OrgVdcs) == 0 {
		return nil, fmt.Errorf("expected at least one VDC in cluster '%s': %s", d.Id(), err)
	}

	vdc, err := adminOrg.GetVDCByName(cluster.Capvcd.Status.Capvcd.VcdProperties.OrgVdcs[0].Name, false)
	if err != nil {
		return nil, fmt.Errorf("could not get VDC with name %s: %s", cluster.Capvcd.Status.Capvcd.VcdProperties.OrgVdcs[0].Name, err)
	}
	dSet(d, "vdc_id", vdc.Vdc.ID)
	network, err := vdc.GetOrgVdcNetworkByName(cluster.Capvcd.Status.Capvcd.VcdProperties.OrgVdcs[0].OvdcNetworkName, false)
	if err != nil {
		return nil, fmt.Errorf("could not get Org VDC Network with name %s: %s", cluster.Capvcd.Status.Capvcd.VcdProperties.OrgVdcs[0].OvdcNetworkName, err)
	}
	dSet(d, "network_id", network.OrgVDCNetwork.ID)

	if _, ok := d.GetOk("owner"); ok {
		// This field is optional, as it can take the value from the VCD client
		if cluster.Owner == "" {
			return nil, fmt.Errorf("could not retrieve Owner information from Cluster")
		}
		dSet(d, "owner", cluster.Owner)
	}

	if _, ok := d.GetOk("api_token_file"); !ok {
		// During imports, this field is impossible to get, so we set an artificial value, as this argument
		// is required at runtime
		dSet(d, "api_token_file", "******")
	}

	bindings := make([]string, len(cluster.Capvcd.Status.Capvcd.ClusterResourceSetBindings))
	for i, binding := range cluster.Capvcd.Status.Capvcd.ClusterResourceSetBindings {
		bindings[i] = binding.Name
	}
	err = d.Set("cluster_resource_set_bindings", bindings)
	if err != nil {
		return nil, fmt.Errorf("could not set 'cluster_resource_set_bindings': %s", err)
	}

	dSet(d, "cpi_version", cluster.Capvcd.Status.Cpi.Version)
	dSet(d, "csi_version", cluster.Capvcd.Status.Csi.Version)
	dSet(d, "capvcd_version", cluster.Capvcd.Status.Capvcd.CapvcdVersion)
	dSet(d, "kubernetes_version", cluster.Capvcd.Status.Capvcd.Upgrade.Current.KubernetesVersion)
	dSet(d, "tkg_product_version", cluster.Capvcd.Status.Capvcd.Upgrade.Current.TkgVersion)
	if len(cluster.Capvcd.Status.Capvcd.K8sNetwork.Pods.CidrBlocks) == 0 {
		return nil, fmt.Errorf("expected at least one Pod CIDR block in cluster '%s': %s", d.Id(), err)
	}
	dSet(d, "pods_cidr", cluster.Capvcd.Status.Capvcd.K8sNetwork.Pods.CidrBlocks[0])
	if len(cluster.Capvcd.Status.Capvcd.K8sNetwork.Services.CidrBlocks) == 0 {
		return nil, fmt.Errorf("expected at least one Services CIDR block in cluster '%s': %s", d.Id(), err)
	}
	dSet(d, "services_cidr", cluster.Capvcd.Status.Capvcd.K8sNetwork.Services.CidrBlocks[0])

	nodePoolBlocks := make([]map[string]interface{}, len(cluster.Capvcd.Status.Capvcd.NodePool)-1)
	controlPlaneBlocks := make([]map[string]interface{}, 1)
	nameToIds := map[string]string{"": ""} // Initialize with empty value
	for i, nodePool := range cluster.Capvcd.Status.Capvcd.NodePool {
		block := map[string]interface{}{}
		block["machine_count"] = nodePool.DesiredReplicas
		// TODO: This needs a refactoring
		if nodePool.PlacementPolicy != "" {
			policies, err := vcdClient.GetAllVdcComputePoliciesV2(url.Values{
				"filter": []string{fmt.Sprintf("name==%s", nodePool.PlacementPolicy)},
			})
			if err != nil {
				return nil, err // TODO
			}
			nameToIds[nodePool.PlacementPolicy] = policies[0].VdcComputePolicyV2.ID
		}
		if nodePool.SizingPolicy != "" {
			policies, err := vcdClient.GetAllVdcComputePoliciesV2(url.Values{
				"filter": []string{fmt.Sprintf("name==%s", nodePool.SizingPolicy)},
			})
			if err != nil {
				return nil, err // TODO
			}
			nameToIds[nodePool.SizingPolicy] = policies[0].VdcComputePolicyV2.ID
		}
		if nodePool.StorageProfile != "" {
			ref, err := vdc.FindStorageProfileReference(nodePool.StorageProfile)
			if err != nil {
				return nil, fmt.Errorf("could not get Default Storage Class options from 'spec.vcdKe.defaultStorageClassOptions': %s", err) // TODO
			}
			nameToIds[nodePool.StorageProfile] = ref.ID
		}
		block["sizing_policy_id"] = nameToIds[nodePool.SizingPolicy]
		if nodePool.NvidiaGpuEnabled { // TODO: Be sure this is a worker node pool and not control plane (doesnt have this attr)
			block["vgpu_policy_id"] = nameToIds[nodePool.PlacementPolicy] // It's a placement policy here
		} else {
			block["placement_policy_id"] = nameToIds[nodePool.PlacementPolicy]
		}
		block["storage_profile_id"] = nameToIds[nodePool.StorageProfile]
		block["disk_size_gi"] = nodePool.DiskSizeMb / 1024

		if strings.HasSuffix(nodePool.Name, "-control-plane-node-pool") {
			// Control Plane
			if len(cluster.Capvcd.Status.Capvcd.ClusterApiStatus.ApiEndpoints) == 0 {
				return nil, fmt.Errorf("could not retrieve Cluster IP")
			}
			block["ip"] = cluster.Capvcd.Status.Capvcd.ClusterApiStatus.ApiEndpoints[0].Host
			controlPlaneBlocks[0] = block
		} else {
			// Worker node
			block["name"] = nodePool.Name

			nodePoolBlocks[i] = block
		}
	}
	err = d.Set("node_pool", nodePoolBlocks)
	if err != nil {
		return nil, fmt.Errorf("could not set 'node_pool' pools: %s", err)
	}
	err = d.Set("control_plane", controlPlaneBlocks)
	if err != nil {
		return nil, fmt.Errorf("could not set 'control_plane': %s", err)
	}

	defaultStorageClassOptions, err := traverseMapAndGet[map[string]interface{}](rde.DefinedEntity.Entity, "spec.vcdKe.defaultStorageClassOptions")
	var defaultStorageClass []map[string]interface{}
	if err != nil {
		if !strings.Contains(err.Error(), "does not exist in input map") {
			return nil, fmt.Errorf("could not get Default Storage Class options from 'spec.vcdKe.defaultStorageClassOptions': %s", err)
		}
		// The object does not exist, hence the cluster does not use a default storage class
	} else {
		reclaimPolicy := "retain"
		if defaultStorageClassOptions["useDeleteReclaimPolicy"].(bool) {
			reclaimPolicy = "delete"
		}

		ref, err := vdc.FindStorageProfileReference(defaultStorageClassOptions["vcdStorageProfileName"].(string))
		if err != nil {
			return nil, fmt.Errorf("could not get Default Storage Class options from 'spec.vcdKe.defaultStorageClassOptions': %s", err) // TODO
		}
		// defaultStorageClassOptions["vcdStorageProfileName"]

		defaultStorageClass = append(defaultStorageClass, map[string]interface{}{
			"storage_profile_id": ref.ID,
			"name":               defaultStorageClassOptions["k8sStorageClassName"],
			"reclaim_policy":     reclaimPolicy,
			"filesystem":         defaultStorageClassOptions["filesystem"],
		})

	}
	err = d.Set("default_storage_class", defaultStorageClass)
	if err != nil {
		return nil, fmt.Errorf("could not save 'default_storage_class': %s", err)
	}

	state, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
	if err != nil {
		return nil, fmt.Errorf("could not read 'status.vcdKe.state' from Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	dSet(d, "state", state)

	if state == "provisioned" {
		behaviorVersion := supportedCseVersions[cseVersion][2]

		// This can only be done if the cluster is in 'provisioned' state
		invocationResult := map[string]interface{}{}
		err := rde.InvokeBehaviorAndMarshal(fmt.Sprintf("urn:vcloud:behavior-interface:getFullEntity:cse:capvcd:%s", behaviorVersion), types.BehaviorInvocation{}, &invocationResult)
		if err != nil {
			return nil, fmt.Errorf("could not invoke the behavior to obtain the Kubeconfig for the Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}

		kubeconfig, err := traverseMapAndGet[string](invocationResult, "entity.status.capvcd.private.kubeConfig")
		if err != nil {
			return nil, fmt.Errorf("could not retrieve Kubeconfig for Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}
		dSet(d, "kubeconfig", kubeconfig)
	} else {
		warnings = append(warnings, fmt.Errorf("the Kubernetes cluster with ID '%s' is in '%s' state, won't be able to retrieve the Kubeconfig", d.Id(), state))
	}

	// TODO: Missing ova_id, ssh_public_key, virtual_ip_subnet, auto_repair_on_errors, node_health_check, persistent_volumes

	return warnings, nil
}
