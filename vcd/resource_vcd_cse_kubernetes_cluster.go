package vcd

import (
	"bytes"
	"context"
	_ "embed"
	"encoding/json"
	"fmt"
	"github.com/hashicorp/go-cty/cty"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"github.com/vmware/go-vcloud-director/v2/govcd"
	"github.com/vmware/go-vcloud-director/v2/types/v56"
	"github.com/vmware/go-vcloud-director/v2/util"
	"gopkg.in/yaml.v2"
	"strconv"
	"strings"
	"text/template"
	"time"
)

// TODO: Split per CSE version: 4.1, 4.2...
//
//go:embed cse/rde.tmpl
var cseRdeJsonTemplate string

//go:embed cse/capi-yaml/cluster.tmpl
var cseClusterYamlTemplate string

//go:embed cse/capi-yaml/node_pool.tmpl
var cseNodePoolTemplate string

// Map of CSE version -> [VCDKEConfig RDE Type version, CAPVCD RDE Type version, CAPVCD Behavior version]
var cseVersions = map[string][]string{
	"4.2": {"1.1.0", "1.2.0", "1.0.0"},
}

var resourceVcdCseKubernetesClusterSchema = map[string]*schema.Schema{
	"cse_version": {
		Type:         schema.TypeString,
		Required:     true,
		ForceNew:     true,
		ValidateFunc: validation.StringInSlice(getKeys(cseVersions), false),
		Description:  "The CSE version to use",
	},
	"runtime": {
		Type:         schema.TypeString,
		Optional:     true,
		Default:      "tkg",
		ForceNew:     true,
		ValidateFunc: validation.StringInSlice([]string{"tkg"}, false), // May add others in future releases of CSE
		Description:  "The Kubernetes runtime for the cluster. Only 'tkg' (Tanzu Kubernetes Grid) is supported",
	},
	"name": {
		Type:        schema.TypeString,
		Required:    true,
		ForceNew:    true,
		Description: "The name of the Kubernetes cluster",
		ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
			"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
	},
	"ova_id": {
		Type:        schema.TypeString,
		Required:    true,
		ForceNew:    true,
		Description: "The ID of the vApp Template that corresponds to a Kubernetes template OVA",
	},
	"org": {
		Type:     schema.TypeString,
		Optional: true,
		ForceNew: true,
		Description: "The name of organization that will own this Kubernetes cluster, optional if defined at provider " +
			"level. Useful when connected as sysadmin working across different organizations",
	},
	"vdc_id": {
		Type:        schema.TypeString,
		Required:    true,
		ForceNew:    true,
		Description: "The ID of the VDC that hosts the Kubernetes cluster",
	},
	"network_id": {
		Type:        schema.TypeString,
		Required:    true,
		ForceNew:    true,
		Description: "The ID of the network that the Kubernetes cluster will use",
	},
	"owner": {
		Type:        schema.TypeString,
		Optional:    true,
		ForceNew:    true,
		Description: "The user that creates the cluster and owns the API token specified in 'api_token'. It must have the 'Kubernetes Cluster Author' role. If not specified, it assumes it's the user from the provider configuration",
	},
	"api_token_file": {
		Type:        schema.TypeString,
		Required:    true,
		ForceNew:    true,
		Description: "A file generated by 'vcd_api_token' resource, that stores the API token used to create and manage the cluster, owned by the user specified in 'owner'. Be careful about this file, as it contains sensitive information",
	},
	"ssh_public_key": {
		Type:        schema.TypeString,
		Optional:    true,
		ForceNew:    true,
		Description: "The SSH public key used to login into the cluster nodes",
	},
	"control_plane": {
		Type:     schema.TypeList,
		MaxItems: 1,
		Required: true,
		Elem: &schema.Resource{
			Schema: map[string]*schema.Schema{
				"machine_count": {
					Type:        schema.TypeInt,
					Optional:    true,
					Default:     3, // As suggested in UI
					Description: "The number of nodes that the control plane has. Must be an odd number and higher than 0",
					ValidateDiagFunc: func(v interface{}, path cty.Path) diag.Diagnostics {
						value, ok := v.(int)
						if !ok {
							return diag.Errorf("could not parse int value '%v' for control plane nodes", v)
						}
						if value < 1 || value%2 == 0 {
							return diag.Errorf("number of control plane nodes must be odd and higher than 0, but it was '%d'", value)
						}
						return nil
					},
				},
				"disk_size_gi": {
					Type:             schema.TypeInt,
					Optional:         true,
					Default:          20, // As suggested in UI
					ForceNew:         true,
					ValidateDiagFunc: minimumValue(20, "disk size in Gibibytes must be at least 20"),
					Description:      "Disk size, in Gibibytes, for the control plane nodes. Must be at least 20",
				},
				"sizing_policy_id": {
					Type:        schema.TypeString,
					Optional:    true,
					ForceNew:    true,
					Description: "VM Sizing policy for the control plane nodes",
				},
				"placement_policy_id": {
					Type:        schema.TypeString,
					Optional:    true,
					ForceNew:    true,
					Description: "VM Placement policy for the control plane nodes",
				},
				"storage_profile_id": {
					Type:        schema.TypeString,
					Optional:    true,
					ForceNew:    true,
					Description: "Storage profile for the control plane nodes",
				},
				"ip": {
					Type:         schema.TypeString,
					Optional:     true,
					ForceNew:     true,
					Description:  "IP for the control plane",
					ValidateFunc: checkEmptyOrSingleIP(),
				},
			},
		},
	},
	"node_pool": {
		Type:     schema.TypeSet,
		Required: true,
		MinItems: 1,
		Elem: &schema.Resource{
			Schema: map[string]*schema.Schema{
				"name": {
					Type:        schema.TypeString,
					Required:    true,
					Description: "The name of this node pool",
					ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
						"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
				},
				"machine_count": {
					Type:             schema.TypeInt,
					Optional:         true,
					Default:          1, // As suggested in UI
					Description:      "The number of nodes that this node pool has. Must be higher than 0",
					ValidateDiagFunc: minimumValue(1, "number of nodes must be higher than 0"),
				},
				"disk_size_gi": {
					Type:             schema.TypeInt,
					Optional:         true,
					Default:          20, // As suggested in UI
					ForceNew:         true,
					Description:      "Disk size, in Gibibytes, for the control plane nodes",
					ValidateDiagFunc: minimumValue(20, "disk size in Gibibytes must be at least 20"),
				},
				"sizing_policy_id": {
					Type:        schema.TypeString,
					Optional:    true,
					ForceNew:    true,
					Description: "VM Sizing policy for the control plane nodes",
				},
				"placement_policy_id": {
					Type:        schema.TypeString,
					Optional:    true,
					ForceNew:    true,
					Description: "VM Placement policy for the control plane nodes",
				},
				"vgpu_policy_id": {
					Type:        schema.TypeString,
					Optional:    true,
					ForceNew:    true,
					Description: "vGPU policy for the control plane nodes",
				},
				"storage_profile_id": {
					Type:        schema.TypeString,
					Optional:    true,
					ForceNew:    true,
					Description: "Storage profile for the control plane nodes",
				},
			},
		},
	},
	"default_storage_class": {
		Type:     schema.TypeList,
		MaxItems: 1,
		Optional: true,
		Elem: &schema.Resource{
			Schema: map[string]*schema.Schema{
				"storage_profile_id": {
					Required:    true,
					Type:        schema.TypeString,
					Description: "ID of the storage profile to use for the storage class",
				},
				"name": {
					Required:    true,
					Type:        schema.TypeString,
					Description: "Name to give to this storage class",
					ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
						"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
				},
				"reclaim_policy": {
					Required:     true,
					Type:         schema.TypeString,
					ValidateFunc: validation.StringInSlice([]string{"delete", "retain"}, false),
					Description:  "'delete' deletes the volume when the PersistentVolumeClaim is deleted. 'retain' does not, and the volume can be manually reclaimed",
				},
				"filesystem": {
					Required:     true,
					Type:         schema.TypeString,
					ValidateFunc: validation.StringInSlice([]string{"ext4", "xfs"}, false),
					Description:  "Filesystem of the storage class, can be either 'ext4' or 'xfs'",
				},
			},
		},
	},
	"pods_cidr": {
		Type:        schema.TypeString,
		Optional:    true,
		Default:     "100.96.0.0/11", // As suggested in UI
		Description: "CIDR that the Kubernetes pods will use",
	},
	"services_cidr": {
		Type:        schema.TypeString,
		Optional:    true,
		Default:     "100.64.0.0/13", // As suggested in UI
		Description: "CIDR that the Kubernetes services will use",
	},
	"virtual_ip_subnet": {
		Type:        schema.TypeString,
		Optional:    true,
		Description: "Virtual IP subnet for the cluster",
	},
	"auto_repair_on_errors": {
		Type:        schema.TypeBool,
		Optional:    true,
		Default:     false,
		Description: "If errors occur before the Kubernetes cluster becomes available, and this argument is 'true', CSE Server will automatically attempt to repair the cluster",
	},
	"node_health_check": {
		Type:        schema.TypeBool,
		Optional:    true,
		Default:     false,
		Description: "After the Kubernetes cluster becomes available, nodes that become unhealthy will be remediated according to unhealthy node conditions and remediation rules",
	},
	"create_timeout_minutes": {
		Type:             schema.TypeInt,
		Optional:         true,
		Default:          60,
		Description:      "The time, in minutes, to wait for the cluster to be completely created, with a ready-to-use Kubeconfig. 0 means wait indefinitely",
		ValidateDiagFunc: minimumValue(0, "timeout must be at least 0 (no timeout)"),
	},
	"delete_timeout_minutes": {
		Type:             schema.TypeInt,
		Optional:         true,
		Default:          10,
		Description:      "The time, in minutes, to wait for the cluster to be deleted when it is marked for deletion. 0 means wait indefinitely",
		ValidateDiagFunc: minimumValue(0, "timeout must be at least 0 (no timeout)"),
	},
	"state": {
		Type:        schema.TypeString,
		Computed:    true,
		Description: "The state of the cluster, can be 'provisioning', 'provisioned', 'deleting' or 'error'. Useful to check whether the Kubernetes cluster is in a stable status",
	},
	"kubeconfig": {
		Type:        schema.TypeString,
		Computed:    true,
		Description: "The contents of the kubeconfig of the Kubernetes cluster, only available when 'state=provisioned'",
	},
	"raw_cluster_rde_json": {
		Type:        schema.TypeString,
		Computed:    true,
		Description: "The raw JSON that describes the cluster configuration inside the Runtime Defined Entity",
	},
}

func resourceVcdCseKubernetesCluster() *schema.Resource {
	return &schema.Resource{
		CreateContext: resourceVcdCseKubernetesClusterCreate,
		ReadContext:   resourceVcdCseKubernetesRead,
		UpdateContext: resourceVcdCseKubernetesUpdate,
		DeleteContext: resourceVcdCseKubernetesDelete,
		Schema:        resourceVcdCseKubernetesClusterSchema,
	}
}

// getCseRdeTypeVersions gets the RDE Type versions. First returned parameter is VCDKEConfig, second is CAPVCDCluster, third is CAPVCD Behavior version
func getCseRdeTypeVersions(d *schema.ResourceData) (string, string, string) {
	versions := cseVersions[d.Get("cse_version").(string)]
	return versions[0], versions[1], versions[2]
}

func resourceVcdCseKubernetesClusterCreate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)
	vcdKeConfigRdeTypeVersion, capvcdClusterRdeTypeVersion, _ := getCseRdeTypeVersions(d)

	clusterDetails, err := getClusterCreateDto(d, vcdClient, vcdKeConfigRdeTypeVersion, capvcdClusterRdeTypeVersion)
	if err != nil {
		return diag.Errorf("could not create Kubernetes cluster: %s", err)
	}

	entityMap, err := getCseKubernetesClusterEntityMap(d, clusterDetails)
	if err != nil {
		return diag.Errorf("could not create Kubernetes cluster with name '%s': %s", clusterDetails.Name, err)
	}

	rde, err := clusterDetails.RdeType.CreateRde(types.DefinedEntity{
		EntityType: clusterDetails.RdeType.DefinedEntityType.ID,
		Name:       clusterDetails.Name,
		Entity:     entityMap,
	}, &govcd.TenantContext{
		OrgId:   clusterDetails.Org.AdminOrg.ID,
		OrgName: clusterDetails.Org.AdminOrg.Name,
	})
	if err != nil {
		return diag.Errorf("could not create Kubernetes cluster with name '%s': %s", clusterDetails.Name, err)
	}

	// We need to set the ID here to be able to distinguish this cluster from all the others that may have the same name and RDE Type.
	// We could use some other ways of filtering, but ID is the only accurate.
	// Also, the RDE is created at this point, so Terraform should trigger an update/delete next.
	// If the cluster can't be created due to errors, users should delete it and retry, like in UI.
	d.SetId(rde.DefinedEntity.ID)

	state, err := waitUntilClusterIsProvisioned(vcdClient, d, rde.DefinedEntity.ID)
	if err != nil {
		return diag.Errorf("Kubernetes cluster creation failed: %s", err)
	}
	if state != "provisioned" {
		return diag.Errorf("Kubernetes cluster creation failed, cluster is not in 'provisioned' state, but '%s'", state)
	}

	return resourceVcdCseKubernetesRead(ctx, d, meta)
}

// waitUntilClusterIsProvisioned waits for the Kubernetes cluster to be in "provisioned" state, either indefinitely (if "create_timeout_minutes=0")
// or until this timeout is reached. If one of the states is "error", this function also checks whether "auto_repair_on_errors=true" to keep
// waiting.
func waitUntilClusterIsProvisioned(vcdClient *VCDClient, d *schema.ResourceData, rdeId string) (string, error) {
	var elapsed time.Duration
	timeout := d.Get("create_timeout_minutes").(int)
	currentState := ""

	start := time.Now()
	for elapsed <= time.Duration(timeout)*time.Minute || timeout == 0 { // If the user specifies create_timeout_minutes=0, we wait forever
		rde, err := vcdClient.GetRdeById(rdeId)
		if err != nil {
			return "", err
		}
		currentState, err = traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
		if err != nil {
			util.Logger.Printf("[DEBUG] Failed getting cluster state: %s", err)
			// We ignore this error, as eventually the state should be populated
		} else {

			// Add some traceability in the logs and Terraform output about the progress of the cluster provisioning
			eventSet, err := traverseMapAndGet[[]interface{}](rde.DefinedEntity.Entity, "status.vcdKe.eventSet")
			if err == nil {
				latestEvent, err := traverseMapAndGet[string](eventSet[len(eventSet)-1], "additionalDetails.Detailed Event")
				if err != nil {
					util.Logger.Printf("[DEBUG] waiting for cluster to be provisioned. Latest event: '%s'", latestEvent)
				}
			}

			switch currentState {
			case "provisioned":
				return currentState, nil
			case "error":
				// We just finish if auto-recovery is disabled, otherwise we just let CSE fixing things in background
				if !d.Get("auto_repair_on_errors").(bool) {
					// Try to give feedback about what went wrong, which is located in a set of events in the RDE payload
					latestError := "could not parse error event"
					errorSet, err := traverseMapAndGet[[]interface{}](rde.DefinedEntity.Entity, "status.capvcd.errorSet")
					if err == nil {
						latestError, err = traverseMapAndGet[string](errorSet[len(errorSet)-1], "additionalDetails.error")
						if err != nil {
							latestError = "could not parse error event"
						}
					}
					return "", fmt.Errorf("got an error and 'auto_repair_on_errors=false', aborting. Latest error: %s", latestError)
				}
			}
		}

		elapsed = time.Since(start)
		time.Sleep(50 * time.Second)

	}
	return "", fmt.Errorf("timeout of %d minutes reached, latest cluster state obtained was '%s'", timeout, currentState)
}

func resourceVcdCseKubernetesRead(_ context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)
	var diags diag.Diagnostics
	_, _, capvcdBehaviorVersion := getCseRdeTypeVersions(d)

	// The ID must be already set for the read to be successful. We can't rely on GetRdesByName as there can be
	// many clusters with the same name and RDE Type.
	var err error
	rde, err := vcdClient.GetRdeById(d.Id())
	if err != nil {
		return diag.Errorf("could not read Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}

	state, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
	if err != nil {
		return diag.Errorf("could not read Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	dSet(d, "state", state)

	if state == "provisioned" {
		// This can only be done if the cluster is in 'provisioned' state
		invocationResult := map[string]interface{}{}
		err := rde.InvokeBehaviorAndMarshal(fmt.Sprintf("urn:vcloud:behavior-interface:getFullEntity:cse:capvcd:%s", capvcdBehaviorVersion), types.BehaviorInvocation{}, invocationResult)
		if err != nil {
			return diag.Errorf("could not invoke the behavior to obtain the Kubeconfig for the Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}

		kubeconfig, err := traverseMapAndGet[string](invocationResult, "entity.status.capvcd.private.kubeConfig")
		if err != nil {
			return diag.Errorf("could not retrieve Kubeconfig for Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}
		dSet(d, "kubeconfig", kubeconfig)
	} else {
		diags = append(diags, diag.Diagnostic{
			Severity: diag.Warning,
			Summary:  "Kubernetes cluster not in 'provisioned' state",
			Detail:   fmt.Sprintf("Kubernetes cluster with ID '%s' is in '%s' state, won't be able to retrieve the Kubeconfig", d.Id(), state),
		})
	}

	jsonEntity, err := jsonToCompactString(rde.DefinedEntity.Entity)
	if err != nil {
		diags = append(diags, diag.Errorf("could not save the cluster '%s' raw RDE contents into 'raw_cluster_rde_json' attribute: %s", rde.DefinedEntity.ID, err)...)
	}
	if diags != nil && diags.HasError() {
		return diags
	}
	dSet(d, "raw_cluster_rde_json", jsonEntity)

	d.SetId(rde.DefinedEntity.ID) // ID is already there, but just for completeness/readability
	return nil
}

func resourceVcdCseKubernetesUpdate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)

	// The ID must be already set for the read to be successful. We can't rely on GetRdesByName as there can be
	// many clusters with the same name and RDE Type.
	rde, err := vcdClient.GetRdeById(d.Id())
	if err != nil {
		return diag.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	state, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
	if err != nil {
		return diag.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	if state != "provisioned" {
		return diag.Errorf("could not update the Kubernetes cluster with ID '%s': It is in '%s' state, but should be 'provisioned'", d.Id(), state)
	}
	// Only OVA and pool sizes can be changed. This is guaranteed by all ForceNew flags, but it's worth it to
	// double-check
	if d.HasChangesExcept("ova_id", "control_plane.0.machine_count", "node_pool") {
		return diag.Errorf("only the Kubernetes template or the control plane/node machine pools can be modified")
	}

	// Gets and unmarshals the CAPI YAML to update it
	capiYaml, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "spec.capiYaml")
	if err != nil {
		return diag.Errorf("could not retrieve the CAPI YAML from the Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	capiMap := map[string]interface{}{}
	err = yaml.Unmarshal([]byte(capiYaml), &capiMap)
	if err != nil {
		return diag.Errorf("could not unmarshal the CAPI YAML from the Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}

	// TODO: Change YAML here
	if d.HasChange("ova_id") {
		newOva := d.Get("ova_id")
		_, err := vcdClient.GetVAppTemplateById(newOva.(string))
		if err != nil {
			return diag.Errorf("could not retrieve the new Kubernetes OVA with ID '%s': %s", newOva, err)
		}
		// TODO: Check whether the update can be performed
	}
	if d.HasChange("control_plane.0.machine_count") {
		util.Logger.Printf("not done but make static complains :)")
	}
	if d.HasChange("node_pool") {
		util.Logger.Printf("not done but make static complains :)")
	}

	updatedYaml := capiYaml // FIXME
	rde.DefinedEntity.Entity["spec"].(map[string]interface{})["capiYaml"] = updatedYaml

	// FIXME: This must be done with retries due to ETag clash
	err = rde.Update(*rde.DefinedEntity)
	if err != nil {
		return diag.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}

	return resourceVcdCseKubernetesRead(ctx, d, meta)
}

// resourceVcdCseKubernetesDelete deletes a CSE Kubernetes cluster. To delete a Kubernetes cluster, one must send
// the flags "markForDelete" and "forceDelete" back to true, so the CSE Server is able to delete all cluster elements
// and perform a cleanup. Hence, this function sends an update of just these two properties and waits for the cluster RDE
// to be gone.
func resourceVcdCseKubernetesDelete(_ context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)
	vcdKe := map[string]interface{}{}

	var elapsed time.Duration
	timeout := d.Get("delete_timeout_minutes").(int)
	start := time.Now()
	for elapsed <= time.Duration(timeout)*time.Minute || timeout == 0 { // If the user specifies delete_timeout_minutes=0, we wait forever
		rde, err := vcdClient.GetRdeById(d.Id())
		if err != nil {
			if govcd.ContainsNotFound(err) {
				return nil // The RDE is gone, so the process is completed and there's nothing more to do
			}
			return diag.Errorf("could not retrieve the Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}

		vcdKe, err = traverseMapAndGet[map[string]interface{}](rde.DefinedEntity.Entity, "spec.vcdKe")
		if err != nil {
			return diag.Errorf("JSON object 'spec.vcdKe' is not correct in the RDE '%s': %s", d.Id(), err)
		}

		if !vcdKe["markForDelete"].(bool) || !vcdKe["forceDelete"].(bool) {
			// Mark the cluster for deletion
			vcdKe["markForDelete"] = true
			vcdKe["forceDelete"] = true
			rde.DefinedEntity.Entity["spec"].(map[string]interface{})["vcdKe"] = vcdKe
			err = rde.Update(*rde.DefinedEntity)
			if err != nil {
				if strings.Contains(strings.ToLower(err.Error()), "etag") {
					continue // We ignore any ETag error. This just means a clash between CSE Server and Terraform, we just try again
				}
				return diag.Errorf("could not mark the Kubernetes cluster with ID '%s' to be deleted: %s", d.Id(), err)
			}
		}

		time.Sleep(30 * time.Second)
		elapsed = time.Since(start)
	}

	// We give a hint to the user whenever possible
	if len(vcdKe) >= 2 && vcdKe["markForDelete"].(bool) && vcdKe["forceDelete"].(bool) {
		return diag.Errorf("timeout of %d minutes reached, the cluster was successfully marked for deletion but was not removed in time", timeout)
	}
	return diag.Errorf("timeout of %d minutes reached, the cluster was not marked for deletion, please try again", timeout)
}

// getCseKubernetesClusterEntityMap gets the payload for the RDE that manages the Kubernetes cluster, so it
// can be created or updated.
func getCseKubernetesClusterEntityMap(d *schema.ResourceData, clusterDetails *createClusterDto) (map[string]interface{}, error) {
	capiYaml, err := generateCapiYaml(d, clusterDetails)
	if err != nil {
		return nil, err
	}

	args := map[string]string{
		"Name":               clusterDetails.Name,
		"Org":                clusterDetails.Org.AdminOrg.Name,
		"VcdUrl":             clusterDetails.VcdUrl,
		"Vdc":                clusterDetails.VdcName,
		"Delete":             "false",
		"ForceDelete":        "false",
		"AutoRepairOnErrors": strconv.FormatBool(d.Get("auto_repair_on_errors").(bool)),
		"ApiToken":           clusterDetails.ApiToken,
		"CapiYaml":           capiYaml,
	}

	if _, isStorageClassSet := d.GetOk("default_storage_class"); isStorageClassSet {
		args["DefaultStorageClassStorageProfile"] = clusterDetails.UrnToNamesCache[d.Get("default_storage_class.0.storage_profile_id").(string)]
		args["DefaultStorageClassName"] = d.Get("default_storage_class.0.name").(string)
		if d.Get("default_storage_class.0.reclaim_policy").(string) == "delete" {
			args["DefaultStorageClassUseDeleteReclaimPolicy"] = "true"
		} else {
			args["DefaultStorageClassUseDeleteReclaimPolicy"] = "false"
		}
		args["DefaultStorageClassFileSystem"] = d.Get("default_storage_class.0.filesystem").(string)
	}

	capvcdEmpty := template.Must(template.New(clusterDetails.Name).Parse(cseRdeJsonTemplate))
	buf := &bytes.Buffer{}
	if err := capvcdEmpty.Execute(buf, args); err != nil {
		return nil, fmt.Errorf("could not render the Go template with the CAPVCD JSON: %s", err)
	}

	var result interface{}
	err = json.Unmarshal(buf.Bytes(), &result)
	if err != nil {
		return nil, fmt.Errorf("could not generate a correct CAPVCD JSON: %s", err)
	}

	return result.(map[string]interface{}), nil
}

// generateNodePoolYaml generates YAML blocks corresponding to the Kubernetes node pools.
func generateNodePoolYaml(d *schema.ResourceData, clusterDetails *createClusterDto) (string, error) {
	nodePoolEmptyTmpl := template.Must(template.New(clusterDetails.Name + "_NodePool").Parse(cseNodePoolTemplate))
	resultYaml := ""
	buf := &bytes.Buffer{}

	// We can have many node pool blocks, we build a YAML object for each one of them.
	for _, nodePoolRaw := range d.Get("node_pool").(*schema.Set).List() {
		nodePool := nodePoolRaw.(map[string]interface{})
		name := nodePool["name"].(string)

		// Check the correctness of the compute policies in the node pool block
		placementPolicyId := nodePool["placement_policy_id"]
		vpguPolicyId := nodePool["vgpu_policy_id"]
		if placementPolicyId != "" && vpguPolicyId != "" {
			return "", fmt.Errorf("the node pool '%s' should have either a Placement Policy or a vGPU Policy, not both", name)
		}
		if vpguPolicyId != "" {
			placementPolicyId = vpguPolicyId // For convenience, we just use one of them as both cannot be set at same time
		}

		if err := nodePoolEmptyTmpl.Execute(buf, map[string]string{
			"ClusterName":             clusterDetails.Name,
			"NodePoolName":            name,
			"TargetNamespace":         clusterDetails.Name + "-ns",
			"Catalog":                 clusterDetails.CatalogName,
			"VAppTemplate":            clusterDetails.OvaName,
			"NodePoolSizingPolicy":    clusterDetails.UrnToNamesCache[nodePool["sizing_policy_id"].(string)],
			"NodePoolPlacementPolicy": clusterDetails.UrnToNamesCache[placementPolicyId.(string)],
			"NodePoolStorageProfile":  clusterDetails.UrnToNamesCache[nodePool["storage_profile_id"].(string)],
			"NodePoolDiskSize":        fmt.Sprintf("%dGi", nodePool["disk_size_gi"].(int)),
			"NodePoolEnableGpu":       strconv.FormatBool(vpguPolicyId != ""),
			"NodePoolMachineCount":    strconv.Itoa(nodePool["machine_count"].(int)),
			"KubernetesVersion":       clusterDetails.TkgVersion.KubernetesVersion,
		}); err != nil {
			return "", fmt.Errorf("could not generate a correct Node Pool YAML: %s", err)
		}
		resultYaml += fmt.Sprintf("%s\n---\n", buf.String())
		buf.Reset()
	}
	return resultYaml, nil
}
