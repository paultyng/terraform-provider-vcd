package vcd

import (
	"bytes"
	"context"
	_ "embed"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"github.com/hashicorp/go-cty/cty"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"github.com/vmware/go-vcloud-director/v2/govcd"
	"github.com/vmware/go-vcloud-director/v2/types/v56"
	"github.com/vmware/go-vcloud-director/v2/util"
	"gopkg.in/yaml.v2"
	"os"
	"strconv"
	"strings"
	"text/template"
	"time"
)

// supportedCseVersions is a map that contains only the supported CSE versions as keys,
// and its corresponding components versions as a slice of strings. The first string is the VCDKEConfig RDE Type version,
// then the CAPVCD RDE Type version and finally the CAPVCD Behavior version.
var supportedCseVersions = map[string][]string{
	"4.2": {
		"1.1.0", // VCDKEConfig RDE Type version
		"1.2.0", // CAPVCD RDE Type version
		"1.0.0", // CAPVCD Behavior version
	},
}

func resourceVcdCseKubernetesCluster() *schema.Resource {
	return &schema.Resource{
		CreateContext: resourceVcdCseKubernetesClusterCreate,
		ReadContext:   resourceVcdCseKubernetesRead,
		UpdateContext: resourceVcdCseKubernetesUpdate,
		DeleteContext: resourceVcdCseKubernetesDelete,
		Schema: map[string]*schema.Schema{
			"cse_version": {
				Type:         schema.TypeString,
				Optional:     true, // Required, but validated at runtime
				ForceNew:     true,
				ValidateFunc: validation.StringInSlice(getKeys(supportedCseVersions), false),
				Description:  "The CSE version to use",
			},
			"runtime": {
				Type:         schema.TypeString,
				Optional:     true,
				Default:      "tkg",
				ForceNew:     true,
				ValidateFunc: validation.StringInSlice([]string{"tkg"}, false), // May add others in future releases of CSE
				Description:  "The Kubernetes runtime for the cluster. Only 'tkg' (Tanzu Kubernetes Grid) is supported",
			},
			"name": {
				Type:        schema.TypeString,
				Optional:    true, // Required, but validated at runtime
				ForceNew:    true,
				Description: "The name of the Kubernetes cluster",
				ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
					"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
			},
			"ova_id": {
				Type:        schema.TypeString,
				Optional:    true, // Required, but validated at runtime
				ForceNew:    true,
				Description: "The ID of the vApp Template that corresponds to a Kubernetes template OVA",
			},
			"org": {
				Type:     schema.TypeString,
				Optional: true,
				ForceNew: true,
				Description: "The name of organization that will own this Kubernetes cluster, optional if defined at provider " +
					"level. Useful when connected as sysadmin working across different organizations",
			},
			"vdc_id": {
				Type:        schema.TypeString,
				Optional:    true, // Required, but validated at runtime
				ForceNew:    true,
				Description: "The ID of the VDC that hosts the Kubernetes cluster",
			},
			"network_id": {
				Type:        schema.TypeString,
				Optional:    true, // Required, but validated at runtime
				ForceNew:    true,
				Description: "The ID of the network that the Kubernetes cluster will use",
			},
			"owner": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: "The user that creates the cluster and owns the API token specified in 'api_token'. It must have the 'Kubernetes Cluster Author' role. If not specified, it assumes it's the user from the provider configuration",
			},
			"api_token_file": {
				Type:        schema.TypeString,
				Optional:    true, // Required, but validated at runtime
				ForceNew:    true,
				Description: "A file generated by 'vcd_api_token' resource, that stores the API token used to create and manage the cluster, owned by the user specified in 'owner'. Be careful about this file, as it contains sensitive information",
			},
			"ssh_public_key": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: "The SSH public key used to login into the cluster nodes",
			},
			"control_plane": {
				Type:        schema.TypeList,
				MaxItems:    1,
				Optional:    true, // Required, but validated at runtime
				Description: "Defines the control plane for the cluster",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"machine_count": {
							Type:        schema.TypeInt,
							Optional:    true,
							Default:     3, // As suggested in UI
							Description: "The number of nodes that the control plane has. Must be an odd number and higher than 0",
							ValidateDiagFunc: func(v interface{}, path cty.Path) diag.Diagnostics {
								value, ok := v.(int)
								if !ok {
									return diag.Errorf("could not parse int value '%v' for control plane nodes", v)
								}
								if value < 1 || value%2 == 0 {
									return diag.Errorf("number of control plane nodes must be odd and higher than 0, but it was '%d'", value)
								}
								return nil
							},
						},
						"disk_size_gi": {
							Type:             schema.TypeInt,
							Optional:         true,
							Default:          20, // As suggested in UI
							ForceNew:         true,
							ValidateDiagFunc: minimumValue(20, "disk size in Gibibytes (Gi) must be at least 20"),
							Description:      "Disk size, in Gibibytes (Gi), for the control plane nodes. Must be at least 20",
						},
						"sizing_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Sizing policy for the control plane nodes",
						},
						"placement_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Placement policy for the control plane nodes",
						},
						"storage_profile_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "Storage profile for the control plane nodes",
						},
						"ip": {
							Type:         schema.TypeString,
							Optional:     true,
							Computed:     true,
							ForceNew:     true,
							Description:  "IP for the control plane. It will be automatically assigned during cluster creation if left empty",
							ValidateFunc: checkEmptyOrSingleIP(),
						},
					},
				},
			},
			"node_pool": {
				Type:        schema.TypeSet,
				Optional:    true, // Required, but validated at runtime
				Description: "Defines a node pool for the cluster",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"name": {
							Type:        schema.TypeString,
							Optional:    true, // Required, but validated at runtime
							ForceNew:    true,
							Description: "The name of this node pool",
							ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
								"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
						},
						"machine_count": {
							Type:             schema.TypeInt,
							Optional:         true,
							Default:          1, // As suggested in UI
							Description:      "The number of nodes that this node pool has. Must be higher than 0",
							ValidateDiagFunc: minimumValue(1, "number of nodes must be higher than 0"),
						},
						"disk_size_gi": {
							Type:             schema.TypeInt,
							Optional:         true,
							Default:          20, // As suggested in UI
							ForceNew:         true,
							Description:      "Disk size, in Gibibytes (Gi), for the control plane nodes",
							ValidateDiagFunc: minimumValue(20, "disk size in Gibibytes (Gi) must be at least 20"),
						},
						"sizing_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Sizing policy for the control plane nodes",
						},
						"placement_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "VM Placement policy for the control plane nodes",
						},
						"vgpu_policy_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "vGPU policy for the control plane nodes",
						},
						"storage_profile_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "Storage profile for the control plane nodes",
						},
					},
				},
			},
			"default_storage_class": {
				Type:        schema.TypeList,
				MaxItems:    1,
				Optional:    true,
				Description: "Defines the default storage class for the cluster",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"storage_profile_id": {
							Optional:    true, // Required, but validated at runtime
							ForceNew:    true,
							Type:        schema.TypeString,
							Description: "ID of the storage profile to use for the storage class",
						},
						"name": {
							Optional:    true, // Required, but validated at runtime
							ForceNew:    true,
							Type:        schema.TypeString,
							Description: "Name to give to this storage class",
							ValidateDiagFunc: matchRegex(`^[a-z](?:[a-z0-9-]{0,29}[a-z0-9])?$`, "name must contain only lowercase alphanumeric characters or '-',"+
								"start with an alphabetic character, end with an alphanumeric, and contain at most 31 characters"),
						},
						"reclaim_policy": {
							Optional:     true, // Required, but validated at runtime
							ForceNew:     true,
							Type:         schema.TypeString,
							ValidateFunc: validation.StringInSlice([]string{"delete", "retain"}, false),
							Description:  "'delete' deletes the volume when the PersistentVolumeClaim is deleted. 'retain' does not, and the volume can be manually reclaimed",
						},
						"filesystem": {
							Optional:     true, // Required, but validated at runtime
							ForceNew:     true,
							Type:         schema.TypeString,
							ValidateFunc: validation.StringInSlice([]string{"ext4", "xfs"}, false),
							Description:  "Filesystem of the storage class, can be either 'ext4' or 'xfs'",
						},
					},
				},
			},
			"pods_cidr": {
				Type:        schema.TypeString,
				Optional:    true,
				Default:     "100.96.0.0/11", // As suggested in UI
				Description: "CIDR that the Kubernetes pods will use",
			},
			"services_cidr": {
				Type:        schema.TypeString,
				Optional:    true,
				Default:     "100.64.0.0/13", // As suggested in UI
				Description: "CIDR that the Kubernetes services will use",
			},
			"virtual_ip_subnet": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: "Virtual IP subnet for the cluster",
			},
			"auto_repair_on_errors": {
				Type:        schema.TypeBool,
				Optional:    true,
				Default:     false,
				Description: "If errors occur before the Kubernetes cluster becomes available, and this argument is 'true', CSE Server will automatically attempt to repair the cluster",
			},
			"node_health_check": {
				Type:        schema.TypeBool,
				Optional:    true,
				Default:     false,
				Description: "After the Kubernetes cluster becomes available, nodes that become unhealthy will be remediated according to unhealthy node conditions and remediation rules",
			},
			"operations_timeout_minutes": {
				Type:     schema.TypeInt,
				Optional: true,
				Default:  60,
				Description: "The time, in minutes, to wait for the cluster operations to be successfully completed. For example, during cluster creation/update, it should be in `provisioned`" +
					"state before the timeout is reached, otherwise the operation will return an error. For cluster deletion, this timeout" +
					"specifies the time to wait until the cluster is completely deleted. Setting this argument to `0` means to wait indefinitely",
				ValidateDiagFunc: minimumValue(0, "timeout must be at least 0 (no timeout)"),
			},
			"kubernetes_version": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The version of Kubernetes installed in this cluster",
			},
			"tkg_product_version": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The version of TKG installed in this cluster",
			},
			"capvcd_version": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The version of CAPVCD used by this cluster",
			},
			"cluster_resource_set_bindings": {
				Type:        schema.TypeSet,
				Computed:    true,
				Description: "The cluster resource set bindings of this cluster",
				Elem:        schema.TypeString,
			},
			"cpi_version": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The version of the Cloud Provider Interface used by this cluster",
			},
			"csi_version": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The version of the Container Storage Interface used by this cluster",
			},
			"state": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The state of the cluster, can be 'provisioning', 'provisioned', 'deleting' or 'error'. Useful to check whether the Kubernetes cluster is in a stable status",
			},
			"kubeconfig": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The contents of the kubeconfig of the Kubernetes cluster, only available when 'state=provisioned'",
			},
			"persistent_volumes": {
				Type:        schema.TypeSet,
				Computed:    true,
				Description: "A set of persistent volumes that are present in the cluster, only available when a 'default_storage_class' was provided during cluster creation",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"name": {
							Computed:    true,
							Type:        schema.TypeString,
							Description: "The name of the persistent volume",
						},
						"status": {
							Computed:    true,
							Type:        schema.TypeString,
							Description: "The status of the persistent volume",
						},
						"shared": {
							Computed:    true,
							Type:        schema.TypeString,
							Description: "Whether the persistent volume is shared or not",
						},
						"attached_node_count": {
							Computed:    true,
							Type:        schema.TypeInt,
							Description: "How many nodes are consuming the persistent volume",
						},
						"iops": {
							Computed:    true,
							Type:        schema.TypeInt,
							Description: "I/O operations per second for the persistent volume",
						},
						"size": {
							Computed:    true,
							Type:        schema.TypeInt,
							Description: "Size of the persistent volume",
						},
						"storage_profile": {
							Computed:    true,
							Type:        schema.TypeString,
							Description: "Storage profile name of the persistent volume",
						},
						"owner": {
							Computed:    true,
							Type:        schema.TypeString,
							Description: "Owner of the persistent volume",
						},
					},
				},
			},
		},
	}
}

// validateCseKubernetesClusterSchema validates all the required arguments at runtime. All the required arguments
// are marked as Optional in the schema to facilitate the Import operation, but some of them are actually mandatory.
func validateCseKubernetesClusterSchema(d *schema.ResourceData) diag.Diagnostics {
	var diags diag.Diagnostics
	for _, arg := range []string{"cse_version", "name", "ova_id", "vdc_id", "network_id", "api_token_file", "control_plane", "node_pool"} {
		if _, ok := d.GetOk(arg); !ok {
			diags = append(diags, diag.Errorf("the argument '%s' is required, but no definition was found", arg)...)
		}
	}
	nodePoolsRaw := d.Get("node_pool").(*schema.Set).List()
	for _, nodePoolRaw := range nodePoolsRaw {
		nodePool := nodePoolRaw.(map[string]interface{})
		for _, arg := range []string{"name"} {
			if _, ok := nodePool[arg]; !ok {
				diags = append(diags, diag.Errorf("the argument 'node_pool.%s' is required, but no definition was found", arg)...)
			}
		}
	}
	if _, ok := d.GetOk("default_storage_class"); ok {
		for _, arg := range []string{"storage_profile_id", "name", "reclaim_policy", "filesystem"} {
			if _, ok := d.GetOk("default_storage_class.0." + arg); !ok {
				diags = append(diags, diag.Errorf("the argument 'default_storage_class.%s' is required, but no definition was found", arg)...)
			}
		}
	}

	if len(diags) > 0 {
		return diags
	}
	return nil
}

func resourceVcdCseKubernetesClusterCreate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	diags := validateCseKubernetesClusterSchema(d)
	if diags.HasError() && len(diags) > 0 {
		return diags
	}

	vcdClient := meta.(*VCDClient)
	clusterDetails, err := getClusterCreateDto(d, vcdClient)
	if err != nil {
		return diag.Errorf("could not create Kubernetes cluster: %s", err)
	}

	entityMap, err := getCseKubernetesClusterCreationPayload(d, vcdClient, clusterDetails)
	if err != nil {
		return diag.Errorf("could not create Kubernetes cluster with name '%s': %s", clusterDetails.Name, err)
	}

	rde, err := clusterDetails.RdeType.CreateRde(types.DefinedEntity{
		EntityType: clusterDetails.RdeType.DefinedEntityType.ID,
		Name:       clusterDetails.Name,
		Entity:     entityMap,
	}, &govcd.TenantContext{
		OrgId:   clusterDetails.Org.AdminOrg.ID,
		OrgName: clusterDetails.Org.AdminOrg.Name,
	})
	if err != nil {
		return diag.Errorf("could not create Kubernetes cluster with name '%s': %s", clusterDetails.Name, err)
	}

	// We need to set the ID here to be able to distinguish this cluster from all the others that may have the same name and RDE Type.
	// We could use some other ways of filtering, but ID is the only accurate.
	// Also, the RDE is created at this point, so Terraform should trigger an update/delete next.
	// If the cluster can't be created due to errors, users should delete it and retry, like in UI.
	d.SetId(rde.DefinedEntity.ID)

	state, err := waitUntilClusterIsProvisioned(vcdClient, d, rde.DefinedEntity.ID)
	if err != nil {
		return diag.Errorf("Kubernetes cluster creation failed: %s", err)
	}
	if state != "provisioned" {
		return diag.Errorf("Kubernetes cluster creation failed, cluster is not in 'provisioned' state, but '%s'", state)
	}

	return resourceVcdCseKubernetesRead(ctx, d, meta)
}

func resourceVcdCseKubernetesRead(_ context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)
	var diags diag.Diagnostics
	behaviorVersion := supportedCseVersions[d.Get("cse_version").(string)][2]

	// The ID must be already set for the read to be successful. We can't rely on GetRdesByName as there can be
	// many clusters with the same name and RDE Type.
	var err error
	rde, err := vcdClient.GetRdeById(d.Id())
	if err != nil {
		return diag.Errorf("could not read Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}

	state, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
	if err != nil {
		return diag.Errorf("could not read Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	dSet(d, "state", state)

	if state == "provisioned" {
		// This can only be done if the cluster is in 'provisioned' state
		invocationResult := map[string]interface{}{}
		err := rde.InvokeBehaviorAndMarshal(fmt.Sprintf("urn:vcloud:behavior-interface:getFullEntity:cse:capvcd:%s", behaviorVersion), types.BehaviorInvocation{}, &invocationResult)
		if err != nil {
			return diag.Errorf("could not invoke the behavior to obtain the Kubeconfig for the Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}

		kubeconfig, err := traverseMapAndGet[string](invocationResult, "entity.status.capvcd.private.kubeConfig")
		if err != nil {
			return diag.Errorf("could not retrieve Kubeconfig for Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}
		dSet(d, "kubeconfig", kubeconfig)
	} else {
		diags = append(diags, diag.Diagnostic{
			Severity: diag.Warning,
			Summary:  "Kubernetes cluster not in 'provisioned' state",
			Detail:   fmt.Sprintf("Kubernetes cluster with ID '%s' is in '%s' state, won't be able to retrieve the Kubeconfig", d.Id(), state),
		})
	}

	// TODO: Set
	// kubernetes_version
	// tkg_product_version
	// capvcd_version
	// cluster_resource_set_bindings
	// cpi_version
	// csi_version
	// persistent_volumes

	d.SetId(rde.DefinedEntity.ID) // ID is already there, but just for completeness/readability
	if len(diags) > 0 {
		return diags
	}
	return nil
}

func resourceVcdCseKubernetesUpdate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	diags := validateCseKubernetesClusterSchema(d)
	if diags.HasError() && len(diags) > 0 {
		return diags
	}

	vcdClient := meta.(*VCDClient)

	// Some arguments don't require changes in the backend
	if !d.HasChangesExcept("operations_timeout_minutes") {
		return nil
	}

	// The ID must be already set for the update to be successful. We can't rely on GetRdesByName as there can be
	// many clusters with the same name and RDE Type.
	rde, err := vcdClient.GetRdeById(d.Id())
	if err != nil {
		return diag.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	state, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
	if err != nil {
		return diag.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	if state != "provisioned" {
		return diag.Errorf("could not update the Kubernetes cluster with ID '%s': It is in '%s' state, but should be 'provisioned'", d.Id(), state)
	}

	// Gets and unmarshals the CAPI YAML to update it
	capiYaml, err := traverseMapAndGet[string](rde.DefinedEntity.Entity, "spec.capiYaml")
	if err != nil {
		return diag.Errorf("could not retrieve the CAPI YAML from the Kubernetes cluster with ID '%s': %s", d.Id(), err)
	}
	// TODO: Is there a simpler way?
	dec := yaml.NewDecoder(bytes.NewReader([]byte(capiYaml)))
	var yamlDocs []map[string]interface{}
	i := 0
	for {
		yamlDocs[i] = map[string]interface{}{}
		if dec.Decode(&yamlDocs[i]) != nil {
			break
		}
		i++
	}

	if d.HasChange("ova_id") {
		newOva := d.Get("ova_id")
		ova, err := vcdClient.GetVAppTemplateById(newOva.(string))
		if err != nil {
			return diag.Errorf("could not retrieve the new Kubernetes OVA with ID '%s': %s", newOva, err)
		}
		// TODO: Check whether the update can be performed
		for _, yamlDoc := range yamlDocs {
			if yamlDoc["kind"] == "VCDMachineTemplate" {
				yamlDoc["spec"].(map[string]interface{})["template"].(map[string]interface{})["spec"].(map[string]interface{})["template"] = ova.VAppTemplate.Name
			}
		}
	}
	if d.HasChange("control_plane.0.machine_count") {
		for _, yamlDoc := range yamlDocs {
			if yamlDoc["kind"] == "KubeadmControlPlane" {
				yamlDoc["spec"].(map[string]interface{})["replicas"] = d.Get("control_plane.0.machine_count")
			}
		}
	}
	// The node pools can only be resized
	if d.HasChange("node_pool") {
		for _, nodePoolRaw := range d.Get("node_pool").(*schema.Set).List() {
			nodePool := nodePoolRaw.(map[string]interface{})
			for _, yamlDoc := range yamlDocs {
				if yamlDoc["kind"] == "KubeadmControlPlane" {
					if yamlDoc["metadata"].(map[string]interface{})["name"] == nodePool["name"].(string) {
						yamlDoc["spec"].(map[string]interface{})["replicas"] = nodePool["machine_count"].(int)
					}
				}
			}
		}
	}

	if d.HasChange("node_health_check") {
		oldNhc, newNhc := d.GetChange("node_health_check")
		if oldNhc.(bool) && !newNhc.(bool) {
			toDelete := 0
			for i, yamlDoc := range yamlDocs {
				if yamlDoc["kind"] == "MachineHealthCheck" {
					toDelete = i
				}
			}
			yamlDocs[toDelete] = yamlDocs[len(yamlDocs)-1] // We delete the MachineHealthCheck block by putting the last doc in its place
			yamlDocs = yamlDocs[:len(yamlDocs)-1]          // Then we remove the last doc
		} else {
			// Add the YAML block
			vcdKeConfig, err := getVcdKeConfiguration(d, vcdClient)
			if err != nil {
				return diag.FromErr(err)
			}
			rawYaml, err := generateMemoryHealthCheckYaml(d, vcdClient, *vcdKeConfig, d.Get("name").(string))
			if err != nil {
				return diag.FromErr(err)
			}
			yamlBlock := map[string]interface{}{}
			err = yaml.Unmarshal([]byte(rawYaml), &yamlBlock)
			if err != nil {
				return diag.Errorf("error updating Memory Health Check: %s", err)
			}
			yamlDocs = append(yamlDocs, yamlBlock)
		}
		util.Logger.Printf("not done but make static complains :)")
	}

	updatedYaml, err := yaml.Marshal(yamlDocs)
	if err != nil {
		return diag.Errorf("error updating cluster: %s", err)
	}

	// This must be done with retries due to the possible clash on ETags
	_, err = runWithRetry(
		"update cluster",
		"could not update cluster",
		1*time.Minute,
		nil,
		func() (any, error) {
			rde, err := vcdClient.GetRdeById(d.Id())
			if err != nil {
				return nil, fmt.Errorf("could not update Kubernetes cluster with ID '%s': %s", d.Id(), err)
			}

			rde.DefinedEntity.Entity["spec"].(map[string]interface{})["capiYaml"] = updatedYaml
			rde.DefinedEntity.Entity["spec"].(map[string]interface{})["vcdKe"].(map[string]interface{})["autoRepairOnErrors"] = d.Get("auto_repair_on_errors").(bool)

			err = rde.Update(*rde.DefinedEntity)
			if err != nil {
				return nil, err
			}
			return nil, nil
		},
	)
	if err != nil {
		return diag.FromErr(err)
	}

	state, err = waitUntilClusterIsProvisioned(vcdClient, d, rde.DefinedEntity.ID)
	if err != nil {
		return diag.Errorf("Kubernetes cluster update failed: %s", err)
	}
	if state != "provisioned" {
		return diag.Errorf("Kubernetes cluster update failed, cluster is not in 'provisioned' state, but '%s'", state)
	}

	return resourceVcdCseKubernetesRead(ctx, d, meta)
}

// resourceVcdCseKubernetesDelete deletes a CSE Kubernetes cluster. To delete a Kubernetes cluster, one must send
// the flags "markForDelete" and "forceDelete" back to true, so the CSE Server is able to delete all cluster elements
// and perform a cleanup. Hence, this function sends an update of just these two properties and waits for the cluster RDE
// to be gone.
func resourceVcdCseKubernetesDelete(_ context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	vcdClient := meta.(*VCDClient)
	vcdKe := map[string]interface{}{}

	// The following loop is constantly polling VCD to retrieve the RDE, which has a big JSON inside, so we avoid filling
	// the log with these big payloads.
	_ = os.Setenv("GOVCD_LOG_SKIP_HTTP_RESP", "1")
	defer func() {
		_ = os.Unsetenv("GOVCD_LOG_SKIP_HTTP_RESP")
	}()

	var elapsed time.Duration
	timeout := d.Get("operations_timeout_minutes").(int)
	start := time.Now()
	for elapsed <= time.Duration(timeout)*time.Minute || timeout == 0 { // If the user specifies operations_timeout_minutes=0, we wait forever
		rde, err := vcdClient.GetRdeById(d.Id())
		if err != nil {
			if govcd.ContainsNotFound(err) {
				return nil // The RDE is gone, so the process is completed and there's nothing more to do
			}
			return diag.Errorf("could not retrieve the Kubernetes cluster with ID '%s': %s", d.Id(), err)
		}

		vcdKe, err = traverseMapAndGet[map[string]interface{}](rde.DefinedEntity.Entity, "spec.vcdKe")
		if err != nil {
			return diag.Errorf("JSON object 'spec.vcdKe' is not correct in the RDE '%s': %s", d.Id(), err)
		}

		if !vcdKe["markForDelete"].(bool) || !vcdKe["forceDelete"].(bool) {
			// Mark the cluster for deletion
			vcdKe["markForDelete"] = true
			vcdKe["forceDelete"] = true
			rde.DefinedEntity.Entity["spec"].(map[string]interface{})["vcdKe"] = vcdKe
			err = rde.Update(*rde.DefinedEntity)
			if err != nil {
				if strings.Contains(strings.ToLower(err.Error()), "etag") {
					continue // We ignore any ETag error. This just means a clash between CSE Server and Terraform, we just try again
				}
				return diag.Errorf("could not mark the Kubernetes cluster with ID '%s' to be deleted: %s", d.Id(), err)
			}
		}

		time.Sleep(10 * time.Second)
		elapsed = time.Since(start)
	}

	// We give a hint to the user about the deletion process result
	if len(vcdKe) >= 2 && vcdKe["markForDelete"].(bool) && vcdKe["forceDelete"].(bool) {
		return diag.Errorf("timeout of %d minutes reached, the cluster was successfully marked for deletion but was not removed in time", timeout)
	}
	return diag.Errorf("timeout of %d minutes reached, the cluster was not marked for deletion, please try again", timeout)
}

// getCseKubernetesClusterCreationPayload gets the payload for the RDE that will trigger a Kubernetes cluster creation.
// It generates a valid YAML that is embedded inside the RDE JSON, then it is returned as an unmarshaled
// generic map, that allows to be sent to VCD as it is.
func getCseKubernetesClusterCreationPayload(d *schema.ResourceData, vcdClient *VCDClient, clusterDetails *createClusterDto) (map[string]interface{}, error) {
	capiYaml, err := generateCapiYaml(d, vcdClient, clusterDetails)
	if err != nil {
		return nil, err
	}

	args := map[string]string{
		"Name":               clusterDetails.Name,
		"Org":                clusterDetails.Org.AdminOrg.Name,
		"VcdUrl":             clusterDetails.VcdUrl,
		"Vdc":                clusterDetails.VdcName,
		"Delete":             "false",
		"ForceDelete":        "false",
		"AutoRepairOnErrors": strconv.FormatBool(d.Get("auto_repair_on_errors").(bool)),
		"ApiToken":           clusterDetails.ApiToken,
		"CapiYaml":           capiYaml,
	}

	if _, isStorageClassSet := d.GetOk("default_storage_class"); isStorageClassSet {
		args["DefaultStorageClassStorageProfile"] = clusterDetails.UrnToNamesCache[d.Get("default_storage_class.0.storage_profile_id").(string)]
		args["DefaultStorageClassName"] = d.Get("default_storage_class.0.name").(string)
		if d.Get("default_storage_class.0.reclaim_policy").(string) == "delete" {
			args["DefaultStorageClassUseDeleteReclaimPolicy"] = "true"
		} else {
			args["DefaultStorageClassUseDeleteReclaimPolicy"] = "false"
		}
		args["DefaultStorageClassFileSystem"] = d.Get("default_storage_class.0.filesystem").(string)
	}

	rdeTmpl, err := getCseTemplateFile(d, vcdClient, "rde")
	if err != nil {
		return nil, err
	}

	capvcdEmpty := template.Must(template.New(clusterDetails.Name).Parse(rdeTmpl))
	buf := &bytes.Buffer{}
	if err := capvcdEmpty.Execute(buf, args); err != nil {
		return nil, fmt.Errorf("could not render the Go template with the CAPVCD JSON: %s", err)
	}

	var result interface{}
	err = json.Unmarshal(buf.Bytes(), &result)
	if err != nil {
		return nil, fmt.Errorf("could not generate a correct CAPVCD JSON: %s", err)
	}

	return result.(map[string]interface{}), nil
}

// generateNodePoolYaml generates YAML blocks corresponding to the Kubernetes node pools.
func generateNodePoolYaml(d *schema.ResourceData, vcdClient *VCDClient, clusterDetails *createClusterDto) (string, error) {
	nodePoolTmpl, err := getCseTemplateFile(d, vcdClient, "capiyaml_nodepool")
	if err != nil {
		return "", err
	}

	nodePoolEmptyTmpl := template.Must(template.New(clusterDetails.Name + "-node-pool").Parse(nodePoolTmpl))
	resultYaml := ""
	buf := &bytes.Buffer{}

	// We can have many node pool blocks, we build a YAML object for each one of them.
	for _, nodePoolRaw := range d.Get("node_pool").(*schema.Set).List() {
		nodePool := nodePoolRaw.(map[string]interface{})
		name := nodePool["name"].(string)

		// Check the correctness of the compute policies in the node pool block
		placementPolicyId := nodePool["placement_policy_id"]
		vpguPolicyId := nodePool["vgpu_policy_id"]
		if placementPolicyId != "" && vpguPolicyId != "" {
			return "", fmt.Errorf("the node pool '%s' should have either a Placement Policy or a vGPU Policy, not both", name)
		}
		if vpguPolicyId != "" {
			placementPolicyId = vpguPolicyId // For convenience, we just use one of the variables as both cannot be set at same time
		}

		if err := nodePoolEmptyTmpl.Execute(buf, map[string]string{
			"ClusterName":             clusterDetails.Name,
			"NodePoolName":            name,
			"TargetNamespace":         clusterDetails.Name + "-ns",
			"Catalog":                 clusterDetails.CatalogName,
			"VAppTemplate":            clusterDetails.OvaName,
			"NodePoolSizingPolicy":    clusterDetails.UrnToNamesCache[nodePool["sizing_policy_id"].(string)],
			"NodePoolPlacementPolicy": clusterDetails.UrnToNamesCache[placementPolicyId.(string)], // Can be either Placement or vGPU
			"NodePoolStorageProfile":  clusterDetails.UrnToNamesCache[nodePool["storage_profile_id"].(string)],
			"NodePoolDiskSize":        fmt.Sprintf("%dGi", nodePool["disk_size_gi"].(int)),
			"NodePoolEnableGpu":       strconv.FormatBool(vpguPolicyId != ""),
			"NodePoolMachineCount":    strconv.Itoa(nodePool["machine_count"].(int)),
			"KubernetesVersion":       clusterDetails.TkgVersion.KubernetesVersion,
		}); err != nil {
			return "", fmt.Errorf("could not generate a correct Node Pool YAML: %s", err)
		}
		resultYaml += fmt.Sprintf("%s\n---\n", buf.String())
		buf.Reset()
	}
	return resultYaml, nil
}

// generateMemoryHealthCheckYaml generates a YAML block corresponding to the Kubernetes memory health check.
func generateMemoryHealthCheckYaml(d *schema.ResourceData, vcdClient *VCDClient, vcdKeConfig vcdKeConfig, clusterName string) (string, error) {
	if !d.Get("node_health_check").(bool) {
		return "", nil
	}

	mhcTmpl, err := getCseTemplateFile(d, vcdClient, "capiyaml_mhc")
	if err != nil {
		return "", err
	}

	mhcEmptyTmpl := template.Must(template.New(clusterName + "-mhc").Parse(mhcTmpl))
	buf := &bytes.Buffer{}

	if err := mhcEmptyTmpl.Execute(buf, map[string]string{
		"ClusterName":                clusterName,
		"TargetNamespace":            clusterName + "-ns",
		"MaxUnhealthyNodePercentage": fmt.Sprintf("%.0f%%", vcdKeConfig.MaxUnhealthyNodesPercentage), // With the 'percentage' suffix
		"NodeStartupTimeout":         fmt.Sprintf("%ss", vcdKeConfig.NodeStartupTimeout),             // With the 'second' suffix
		"NodeUnknownTimeout":         fmt.Sprintf("%ss", vcdKeConfig.NodeUnknownTimeout),             // With the 'second' suffix
		"NodeNotReadyTimeout":        fmt.Sprintf("%ss", vcdKeConfig.NodeNotReadyTimeout),            // With the 'second' suffix
	}); err != nil {
		return "", fmt.Errorf("could not generate a correct Memory Health Check YAML: %s", err)
	}
	return fmt.Sprintf("%s\n---\n", buf.String()), nil

}

// waitUntilClusterIsProvisioned waits for the Kubernetes cluster to be in "provisioned" state, either indefinitely (if "operations_timeout_minutes=0")
// or until this timeout is reached. If one of the states is "error", this function also checks whether "auto_repair_on_errors=true" to keep
// waiting.
func waitUntilClusterIsProvisioned(vcdClient *VCDClient, d *schema.ResourceData, rdeId string) (string, error) {
	var elapsed time.Duration
	timeout := d.Get("operations_timeout_minutes").(int)

	// The following loop is constantly polling VCD to retrieve the RDE, which has a big JSON inside, so we avoid filling
	// the log with these big payloads.
	_ = os.Setenv("GOVCD_LOG_SKIP_HTTP_RESP", "1")
	defer func() {
		_ = os.Unsetenv("GOVCD_LOG_SKIP_HTTP_RESP")
	}()
	currentState := ""

	start := time.Now()
	for elapsed <= time.Duration(timeout)*time.Minute || timeout == 0 { // If the user specifies operations_timeout_minutes=0, we wait forever
		rde, err := vcdClient.GetRdeById(rdeId)
		if err != nil {
			return "", err
		}
		currentState, err = traverseMapAndGet[string](rde.DefinedEntity.Entity, "status.vcdKe.state")
		if err != nil {
			util.Logger.Printf("[DEBUG] Failed getting cluster state: %s", err)
			// We ignore this error, as eventually the state should be populated
		} else {

			// Add some traceability in the logs and Terraform output about the progress of the cluster provisioning
			eventSet, err := traverseMapAndGet[[]interface{}](rde.DefinedEntity.Entity, "status.vcdKe.eventSet")
			if err == nil {
				latestEvent, err := traverseMapAndGet[string](eventSet[len(eventSet)-1], "additionalDetails.Detailed Event")
				if err != nil {
					util.Logger.Printf("[DEBUG] waiting for cluster to be provisioned. Latest event: '%s'", latestEvent)
				}
			}

			switch currentState {
			case "provisioned":
				return currentState, nil
			case "error":
				// We just finish if auto-recovery is disabled, otherwise we just let CSE fixing things in background
				if !d.Get("auto_repair_on_errors").(bool) {
					// Try to give feedback about what went wrong, which is located in a set of events in the RDE payload
					latestError := "could not parse error event"
					errorSet, err := traverseMapAndGet[[]interface{}](rde.DefinedEntity.Entity, "status.capvcd.errorSet")
					if err == nil {
						latestError, err = traverseMapAndGet[string](errorSet[len(errorSet)-1], "additionalDetails.error")
						if err != nil {
							latestError = "could not parse error event"
						}
					}
					return "", fmt.Errorf("got an error and 'auto_repair_on_errors=false', aborting. Latest error: %s", latestError)
				}
			}
		}

		elapsed = time.Since(start)
		time.Sleep(30 * time.Second)
	}
	return "", fmt.Errorf("timeout of %d minutes reached, latest cluster state obtained was '%s'", timeout, currentState)
}

// tkgVersionBundle is a type that contains all the versions of the components of
// a Kubernetes cluster that can be obtained with the vApp Template name, downloaded
// from VMware Customer connect:
// https://customerconnect.vmware.com/downloads/details?downloadGroup=TKG-240&productId=1400
type tkgVersionBundle struct {
	EtcdVersion       string
	CoreDnsVersion    string
	TkgVersion        string
	TkrVersion        string
	KubernetesVersion string
}

// getTkgVersionBundleFromVAppTemplateName returns a tkgVersionBundle with the details of
// all the Kubernetes cluster components versions given a valid vApp Template name, that should
// correspond to a Kubernetes template. If it is not a valid vApp Template, returns an error.
func getTkgVersionBundleFromVAppTemplateName(vcdClient *VCDClient, ovaName string) (tkgVersionBundle, error) {
	result := tkgVersionBundle{}

	if strings.Contains(ovaName, "photon") {
		return result, fmt.Errorf("the vApp Template '%s' uses Photon, and it is not supported", ovaName)
	}

	cutPosition := strings.LastIndex(ovaName, "kube-")
	if cutPosition < 0 {
		return result, fmt.Errorf("the vApp Template '%s' is not a Kubernetes template OVA", ovaName)
	}
	parsedOvaName := strings.ReplaceAll(ovaName, ".ova", "")[cutPosition+len("kube-"):]

	// FIXME: This points to my fork, but should point to final version!!
	file, err := fileFromUrlToString(vcdClient, "https://raw.githubusercontent.com/adambarreiro/terraform-provider-vcd/add-cse-cluster-resource/vcd/cse/tkg_versions.json", "json")
	if err != nil {
		return result, fmt.Errorf("error reading tkg_versions.json: %s", err)
	}

	versionsMap := map[string]interface{}{}
	err = json.Unmarshal([]byte(file), &versionsMap)
	if err != nil {
		return result, err
	}
	versionMap, ok := versionsMap[parsedOvaName]
	if !ok {
		return result, fmt.Errorf("the Kubernetes OVA '%s' is not supported", parsedOvaName)
	}

	// The map checking above guarantees that all splits and replaces will work
	result.KubernetesVersion = strings.Split(parsedOvaName, "-")[0]
	result.TkrVersion = strings.ReplaceAll(strings.Split(parsedOvaName, "-")[0], "+", "---") + "-" + strings.Split(parsedOvaName, "-")[1]
	result.TkgVersion = versionMap.(map[string]interface{})["tkg"].(string)
	result.EtcdVersion = versionMap.(map[string]interface{})["etcd"].(string)
	result.CoreDnsVersion = versionMap.(map[string]interface{})["coreDns"].(string)
	return result, nil
}

// createClusterDto is a helper struct that contains all the required elements to successfully create a Kubernetes cluster using CSE.
// This is useful to avoid querying VCD too much, as the Terraform configuration works mostly with IDs, but we require names, among
// other items that we eventually need to retrieve from VCD.
type createClusterDto struct {
	Name            string
	VcdUrl          string
	Org             *govcd.AdminOrg
	VdcName         string
	OvaName         string
	CatalogName     string
	NetworkName     string
	RdeType         *govcd.DefinedEntityType
	UrnToNamesCache map[string]string // Maps unique IDs with their resource names (example: Compute policy ID with its name)
	VcdKeConfig     vcdKeConfig
	TkgVersion      tkgVersionBundle
	Owner           string
	ApiToken        string
}

// vcdKeConfig is a type that contains only the required and relevant fields from the CSE installation configuration,
// such as the Machine Health Check settings or the container registry URL.
type vcdKeConfig struct {
	MaxUnhealthyNodesPercentage float64
	NodeStartupTimeout          string
	NodeNotReadyTimeout         string
	NodeUnknownTimeout          string
	ContainerRegistryUrl        string
}

// getClusterCreateDto creates and returns a createClusterDto object by obtaining all the required information
// from the Terraform resource data and the target VCD.
func getClusterCreateDto(d *schema.ResourceData, vcdClient *VCDClient) (*createClusterDto, error) {
	result := &createClusterDto{}
	result.UrnToNamesCache = map[string]string{"": ""} // Initialize with a "zero" entry, used when there's no ID set in the Terraform schema

	name := d.Get("name").(string)
	result.Name = name

	org, err := vcdClient.GetAdminOrgFromResource(d)
	if err != nil {
		return nil, fmt.Errorf("could not retrieve the cluster Organization: %s", err)
	}
	result.Org = org

	vdcId := d.Get("vdc_id").(string)
	vdc, err := org.GetVDCById(vdcId, true)
	if err != nil {
		return nil, fmt.Errorf("could not retrieve the VDC with ID '%s': %s", vdcId, err)
	}
	result.VdcName = vdc.Vdc.Name

	vAppTemplateId := d.Get("ova_id").(string)
	vAppTemplate, err := vcdClient.GetVAppTemplateById(vAppTemplateId)
	if err != nil {
		return nil, fmt.Errorf("could not retrieve the Kubernetes OVA with ID '%s': %s", vAppTemplateId, err)
	}
	result.OvaName = vAppTemplate.VAppTemplate.Name

	tkgVersions, err := getTkgVersionBundleFromVAppTemplateName(vcdClient, vAppTemplate.VAppTemplate.Name)
	if err != nil {
		return nil, err
	}
	result.TkgVersion = tkgVersions

	catalogName, err := vAppTemplate.GetCatalogName()
	if err != nil {
		return nil, fmt.Errorf("could not retrieve the CatalogName of the OVA '%s': %s", vAppTemplateId, err)
	}
	result.CatalogName = catalogName

	networkId := d.Get("network_id").(string)
	network, err := vdc.GetOrgVdcNetworkById(networkId, true)
	if err != nil {
		return nil, fmt.Errorf("could not retrieve the Org VDC NetworkName with ID '%s': %s", networkId, err)
	}
	result.NetworkName = network.OrgVDCNetwork.Name

	currentCseVersion := supportedCseVersions[d.Get("cse_version").(string)]
	rdeType, err := vcdClient.GetRdeType("vmware", "capvcdCluster", currentCseVersion[1])
	if err != nil {
		return nil, fmt.Errorf("could not retrieve RDE Type vmware:capvcdCluster:'%s': %s", currentCseVersion[1], err)
	}
	result.RdeType = rdeType

	// Fills the cache map that relates IDs of Storage profiles and Compute policies (the schema uses them to build a
	// healthy Terraform dependency graph) with their corresponding names (the cluster YAML and CSE in general uses names only).
	// Having this map minimizes the amount of queries to VCD, specially when building the set of node pools,
	// as there can be a lot of them.
	for _, configBlockAttr := range []string{"default_storage_class", "control_plane", "node_pool"} {
		if _, ok := d.GetOk(configBlockAttr); !ok {
			continue // Some blocks are optional, this is managed by the schema constraints
		}
		// The node_pool is a Set, but the others are already Lists
		var configBlockAsList []interface{}
		if _, isASet := d.Get(configBlockAttr).(*schema.Set); isASet {
			configBlockAsList = d.Get(configBlockAttr).(*schema.Set).List()
		} else {
			configBlockAsList = d.Get(configBlockAttr).([]interface{})
		}

		// For every existing block/list, we check the inner attributes to retrieve their corresponding object names,
		// like Storage Profile names and Compute Policy names. If the ID is already registered, we skip it.
		for _, configBlockRaw := range configBlockAsList {
			configBlock := configBlockRaw.(map[string]interface{})
			if id, ok := configBlock["storage_profile_id"]; ok {
				if _, alreadyPresent := result.UrnToNamesCache[id.(string)]; !alreadyPresent {
					storageProfile, err := vcdClient.GetStorageProfileById(id.(string))
					if err != nil {
						return nil, fmt.Errorf("could not get Storage Profile with ID '%s': %s", id, err)
					}
					result.UrnToNamesCache[id.(string)] = storageProfile.Name
				}
			}
			// The other sub-attributes are just Compute policies, we treat them the same
			for _, attribute := range []string{"sizing_policy_id", "vgpu_policy_id", "placement_policy_id"} {
				id, ok := configBlock[attribute]
				if !ok {
					continue
				}
				if _, alreadyPresent := result.UrnToNamesCache[id.(string)]; alreadyPresent {
					continue
				}
				computePolicy, err := vcdClient.GetVdcComputePolicyV2ById(id.(string))
				if err != nil {
					return nil, fmt.Errorf("could not get Compute Policy with ID '%s': %s", id, err)
				}
				result.UrnToNamesCache[id.(string)] = computePolicy.VdcComputePolicyV2.Name
			}
		}
	}

	vcdKeConfig, err := getVcdKeConfiguration(d, vcdClient)
	if err != nil {
		return nil, err
	}
	result.VcdKeConfig = *vcdKeConfig

	owner, ok := d.GetOk("owner")
	if !ok {
		sessionInfo, err := vcdClient.Client.GetSessionInfo()
		if err != nil {
			return nil, fmt.Errorf("error getting the owner of the cluster: %s", err)
		}
		owner = sessionInfo.User.Name
	}
	result.Owner = owner.(string)

	apiToken, err := govcd.GetTokenFromFile(d.Get("api_token_file").(string))
	if err != nil {
		return nil, fmt.Errorf("API token file could not be parsed or found: %s\nPlease check that the format is the one that 'vcd_api_token' resource uses", err)
	}
	result.ApiToken = apiToken.RefreshToken

	result.VcdUrl = strings.Replace(vcdClient.VCDClient.Client.VCDHREF.String(), "/api", "", 1)
	return result, nil
}

// getVcdKeConfiguration gets the required information from the CSE Server configuration RDE
func getVcdKeConfiguration(d *schema.ResourceData, vcdClient *VCDClient) (*vcdKeConfig, error) {
	currentCseVersion := supportedCseVersions[d.Get("cse_version").(string)]
	result := &vcdKeConfig{}

	rdes, err := vcdClient.GetRdesByName("vmware", "VCDKEConfig", currentCseVersion[0], "vcdKeConfig")
	if err != nil {
		return nil, fmt.Errorf("could not retrieve VCDKEConfig RDE with version %s: %s", currentCseVersion[0], err)
	}
	if len(rdes) != 1 {
		return nil, fmt.Errorf("expected exactly one VCDKEConfig RDE but got %d", len(rdes))
	}

	profiles, err := traverseMapAndGet[[]interface{}](rdes[0].DefinedEntity.Entity, "profiles")
	if err != nil {
		return nil, fmt.Errorf("wrong format of VCDKEConfig, expected a 'profiles' element: %s", err)
	}
	if len(profiles) != 1 {
		return nil, fmt.Errorf("wrong format of VCDKEConfig, expected a single 'profiles' element, got %d", len(profiles))
	}
	// TODO: Check airgapped environments: https://docs.vmware.com/en/VMware-Cloud-Director-Container-Service-Extension/4.1.1a/VMware-Cloud-Director-Container-Service-Extension-Install-provider-4.1.1/GUID-F00BE796-B5F2-48F2-A012-546E2E694400.html
	result.ContainerRegistryUrl = fmt.Sprintf("%s/tkg", profiles[0].(map[string]interface{})["containerRegistryUrl"].(string))

	if _, ok := d.GetOk("node_health_check"); ok {
		mhc, err := traverseMapAndGet[map[string]interface{}](profiles[0], "K8Config.mhc")
		if err != nil {
			return nil, fmt.Errorf("wrong format of VCDKEConfig, expected a 'profiles[0].K8sConfig.mhc' element: %s", err)
		}
		result.MaxUnhealthyNodesPercentage = mhc["maxUnhealthyNodes"].(float64)
		result.NodeStartupTimeout = mhc["nodeStartupTimeout"].(string)
		result.NodeNotReadyTimeout = mhc["nodeUnknownTimeout"].(string)
		result.NodeUnknownTimeout = mhc["nodeNotReadyTimeout"].(string)
	}
	return result, nil
}

// generateCapiYaml generates the YAML string that is required during Kubernetes cluster creation, to be embedded
// in the CAPVCD cluster JSON payload. This function picks data from the Terraform schema and the createClusterDto to
// populate several Go templates and build a final YAML.
func generateCapiYaml(d *schema.ResourceData, vcdClient *VCDClient, clusterDetails *createClusterDto) (string, error) {
	clusterTmpl, err := getCseTemplateFile(d, vcdClient, "capiyaml_cluster")
	if err != nil {
		return "", err
	}

	// This YAML snippet contains special strings, such as "%,", that render wrong using the Go template engine
	sanitizedTemplate := strings.NewReplacer("%", "%%").Replace(clusterTmpl)
	capiYamlEmpty := template.Must(template.New(clusterDetails.Name + "-cluster").Parse(sanitizedTemplate))

	nodePoolYaml, err := generateNodePoolYaml(d, vcdClient, clusterDetails)
	if err != nil {
		return "", err
	}

	memoryHealthCheckYaml, err := generateMemoryHealthCheckYaml(d, vcdClient, clusterDetails.VcdKeConfig, clusterDetails.Name)
	if err != nil {
		return "", err
	}

	args := map[string]string{
		"ClusterName":                 clusterDetails.Name,
		"TargetNamespace":             clusterDetails.Name + "-ns",
		"TkrVersion":                  clusterDetails.TkgVersion.TkrVersion,
		"TkgVersion":                  clusterDetails.TkgVersion.TkgVersion,
		"UsernameB64":                 base64.StdEncoding.EncodeToString([]byte(clusterDetails.Owner)),
		"ApiTokenB64":                 base64.StdEncoding.EncodeToString([]byte(clusterDetails.ApiToken)),
		"PodCidr":                     d.Get("pods_cidr").(string),
		"ServiceCidr":                 d.Get("services_cidr").(string),
		"VcdSite":                     clusterDetails.VcdUrl,
		"Org":                         clusterDetails.Org.AdminOrg.Name,
		"OrgVdc":                      clusterDetails.VdcName,
		"OrgVdcNetwork":               clusterDetails.NetworkName,
		"Catalog":                     clusterDetails.CatalogName,
		"VAppTemplate":                clusterDetails.OvaName,
		"ControlPlaneSizingPolicy":    clusterDetails.UrnToNamesCache[d.Get("control_plane.0.sizing_policy_id").(string)],
		"ControlPlanePlacementPolicy": clusterDetails.UrnToNamesCache[d.Get("control_plane.0.placement_policy_id").(string)],
		"ControlPlaneStorageProfile":  clusterDetails.UrnToNamesCache[d.Get("control_plane.0.storage_profile_id").(string)],
		"ControlPlaneDiskSize":        fmt.Sprintf("%dGi", d.Get("control_plane.0.disk_size_gi").(int)),
		"ControlPlaneMachineCount":    strconv.Itoa(d.Get("control_plane.0.machine_count").(int)),
		"DnsVersion":                  clusterDetails.TkgVersion.CoreDnsVersion,
		"EtcdVersion":                 clusterDetails.TkgVersion.EtcdVersion,
		"ContainerRegistryUrl":        clusterDetails.VcdKeConfig.ContainerRegistryUrl,
		"KubernetesVersion":           clusterDetails.TkgVersion.KubernetesVersion,
		"SshPublicKey":                d.Get("ssh_public_key").(string),
	}
	if _, ok := d.GetOk("control_plane.0.ip"); ok {
		args["ControlPlaneEndpoint"] = d.Get("control_plane.0.ip").(string)
	}
	if _, ok := d.GetOk("virtual_ip_subnet"); ok {
		args["VirtualIpSubnet"] = d.Get("virtual_ip_subnet").(string)
	}

	buf := &bytes.Buffer{}
	if err := capiYamlEmpty.Execute(buf, args); err != nil {
		return "", fmt.Errorf("could not generate a correct CAPI YAML: %s", err)
	}
	// The final "pretty" YAML. To embed it in the final payload it must be marshaled into a one-line JSON string
	prettyYaml := fmt.Sprintf("%s\n%s\n%s", memoryHealthCheckYaml, nodePoolYaml, buf.String())

	// This encoder is used instead of a standard json.Marshal as the YAML contains special
	// characters that are not encoded properly, such as '<'.
	buf.Reset()
	enc := json.NewEncoder(buf)
	enc.SetEscapeHTML(false)
	err = enc.Encode(prettyYaml)
	if err != nil {
		return "", fmt.Errorf("could not encode the CAPI YAML into JSON: %s", err)
	}

	// Removes trailing quotes from the final JSON string
	return strings.Trim(strings.TrimSpace(buf.String()), "\""), nil
}

// getCseTemplateFile gets a Go template file corresponding to the CSE version set in the Terraform configuration
func getCseTemplateFile(d *schema.ResourceData, vcdClient *VCDClient, templateName string) (string, error) {
	cseVersion := d.Get("cse_version").(string)

	// In the future, we can put here some logic for equivalent CSE versions, to avoid duplicating the same Go
	// templates that didn't change among versions.

	// FIXME: This points to my fork, but should point to the final URL!!
	t := fmt.Sprintf("https://raw.githubusercontent.com/adambarreiro/terraform-provider-vcd/add-cse-cluster-resource/vcd/cse/%s/%s.tmpl", cseVersion, templateName)

	return fileFromUrlToString(vcdClient, t, "tmpl")
}
